{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8523d8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "PROJECT_ROOT = r\"C:\\Users\\ADMIN\\Desktop\\NDM_Project\"\n",
    "os.chdir(PROJECT_ROOT)\n",
    "print(\"CWD =\", os.getcwd())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4340ccb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "for p in [\"original_paper\", \"results/phase9\", \"results/cache_phase9\", \"src\", \"tests\"]:\n",
    "    os.makedirs(p, exist_ok=True)\n",
    "print(\"OK folders\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6c7d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install -U numpy scipy pandas matplotlib tqdm pyyaml scikit-learn librosa scikit-image pytest\n",
    "!pip -q install -U torch torchvision torchaudio\n",
    "print(\"OK install\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f848b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile original_paper/protocol.yaml\n",
    "project:\n",
    "  seed: 42\n",
    "\n",
    "data:\n",
    "  cwru_root: data/raw/CWRU\n",
    "  fs_hz: 48000\n",
    "\n",
    "  # 10 lớp: H + B/IR/OR (0.007/0.014/0.021)\n",
    "  # OR chọn @6 để đúng 10 lớp (vì bạn có OR@3/@6/@12)\n",
    "  classes_10:\n",
    "    H: normal_baseline/Normal_1.mat\n",
    "    B007: 48k_drive_end_fault/B007_1.mat\n",
    "    B014: 48k_drive_end_fault/B014_1.mat\n",
    "    B021: 48k_drive_end_fault/B021_1.mat\n",
    "    IR007: 48k_drive_end_fault/IR007_1.mat\n",
    "    IR014: 48k_drive_end_fault/IR014_1.mat\n",
    "    IR021: 48k_drive_end_fault/IR021_1.mat\n",
    "    OR007: 48k_drive_end_fault/OR007@6_1.mat\n",
    "    OR014: 48k_drive_end_fault/OR014@6_1.mat\n",
    "    OR021: 48k_drive_end_fault/OR021@6_1.mat\n",
    "\n",
    "  sample_len: 4710\n",
    "  samples_per_class: 100\n",
    "\n",
    "  split:\n",
    "    train_after_val_per_class: 64\n",
    "    val_per_class: 16\n",
    "    test_per_class: 20\n",
    "\n",
    "preprocess:\n",
    "  normalize: paper  # (x-mean)/max(|x|)\n",
    "  spectrogram:\n",
    "    n_fft: 512\n",
    "    win_length: 512\n",
    "    hop_length: 256   # 50% overlap\n",
    "    mel_bins: 64\n",
    "    log_eps: 1.0e-6\n",
    "    target_frames: 96 # 96x64 đúng VGGish-form\n",
    "\n",
    "features:\n",
    "  shallow:\n",
    "    lbp: { enabled: true, P: 8, R: 1, method: default }\n",
    "    ldp: { enabled: true, k_top: 3 }\n",
    "    lpq: { enabled: true, R_grid: [3, 5, 7] }\n",
    "    mbh_lpq: { enabled: true, R: 7, b_grid: [1,2,4,6,8,10,12] }\n",
    "\n",
    "  deep:\n",
    "    # Nếu máy bạn bị lỗi VGGish do version torchaudio,\n",
    "    # tắt vggish và dùng vgg16 làm deep vẫn chạy được Phase 9.\n",
    "    vggish: { enabled: true }\n",
    "    vgg16:  { enabled: true, layer: fc6 }\n",
    "\n",
    "reduce_and_classify:\n",
    "  pca: { enabled: true, n_components: 128 }\n",
    "  eda: { enabled: true, reg_eps: 1.0e-6 }\n",
    "\n",
    "fusion:\n",
    "  ws_alpha_grid: [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\n",
    "\n",
    "output:\n",
    "  results_dir: results/phase9\n",
    "  cache_dir: results/cache_phase9\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad2f444",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile src/phase9_runner.py\n",
    "import os\n",
    "import json\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List\n",
    "\n",
    "import numpy as np\n",
    "import yaml\n",
    "from scipy.io import loadmat\n",
    "from scipy import linalg\n",
    "from scipy.ndimage import convolve1d\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def ensure_dir(p: str) -> None:\n",
    "    os.makedirs(p, exist_ok=True)\n",
    "\n",
    "\n",
    "def set_seed(seed: int) -> None:\n",
    "    import random\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    try:\n",
    "        import torch\n",
    "        torch.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "\n",
    "def find_1d_signal(mat: Dict) -> np.ndarray:\n",
    "    cand = []\n",
    "    for k, v in mat.items():\n",
    "        if k.startswith(\"__\"):\n",
    "            continue\n",
    "        if isinstance(v, np.ndarray) and np.issubdtype(v.dtype, np.number):\n",
    "            a = np.squeeze(v)\n",
    "            if a.ndim == 1 and a.size >= 1000:\n",
    "                cand.append((a.size, k, a))\n",
    "    if not cand:\n",
    "        raise ValueError(\"Không tìm thấy tín hiệu 1D trong file .mat\")\n",
    "    cand.sort(key=lambda x: x[0], reverse=True)\n",
    "    return cand[0][2].astype(np.float64)\n",
    "\n",
    "\n",
    "def normalize_paper(x: np.ndarray) -> np.ndarray:\n",
    "    x = x.astype(np.float64)\n",
    "    x = x - x.mean()\n",
    "    d = np.max(np.abs(x))\n",
    "    if d < 1e-12:\n",
    "        return x * 0.0\n",
    "    return x / d\n",
    "\n",
    "\n",
    "def segment_waveform(x: np.ndarray, seg_len: int, n_seg: int) -> np.ndarray:\n",
    "    x = np.asarray(x).ravel()\n",
    "    if x.size < seg_len:\n",
    "        x = np.pad(x, (0, seg_len - x.size))\n",
    "    max_start = max(0, x.size - seg_len)\n",
    "    if max_start == 0:\n",
    "        starts = np.zeros(n_seg, dtype=int)\n",
    "    else:\n",
    "        starts = np.linspace(0, max_start, num=n_seg, dtype=int)\n",
    "    return np.stack([x[s:s + seg_len] for s in starts], axis=0)\n",
    "\n",
    "\n",
    "def logmel_paper_like(x: np.ndarray, fs: int, n_fft: int, hop_length: int,\n",
    "                     win_length: int, mel_bins: int, log_eps: float, target_frames: int) -> np.ndarray:\n",
    "    import librosa\n",
    "\n",
    "    S = librosa.feature.melspectrogram(\n",
    "        y=x.astype(np.float32),\n",
    "        sr=fs,\n",
    "        n_fft=n_fft,\n",
    "        hop_length=hop_length,\n",
    "        win_length=win_length,\n",
    "        window=\"hamming\",\n",
    "        center=False,\n",
    "        power=2.0,\n",
    "        n_mels=mel_bins,\n",
    "        fmin=0.0,\n",
    "        fmax=fs / 2,\n",
    "    )\n",
    "    S = np.log(S + log_eps).T  # (T, mel)\n",
    "\n",
    "    T = S.shape[0]\n",
    "    if T != target_frames:\n",
    "        t_old = np.linspace(0.0, 1.0, num=T)\n",
    "        t_new = np.linspace(0.0, 1.0, num=target_frames)\n",
    "        out = np.empty((target_frames, S.shape[1]), dtype=np.float32)\n",
    "        for m in range(S.shape[1]):\n",
    "            out[:, m] = np.interp(t_new, t_old, S[:, m]).astype(np.float32)\n",
    "    else:\n",
    "        out = S.astype(np.float32)\n",
    "\n",
    "    mn, mx = float(out.min()), float(out.max())\n",
    "    if mx - mn > 1e-9:\n",
    "        out = (out - mn) / (mx - mn)\n",
    "    return out.astype(np.float32)  # (96,64)\n",
    "\n",
    "\n",
    "def hist256(code_img: np.ndarray) -> np.ndarray:\n",
    "    h = np.bincount(code_img.ravel().astype(np.int64), minlength=256).astype(np.float64)\n",
    "    s = h.sum()\n",
    "    if s > 0:\n",
    "        h /= s\n",
    "    return h\n",
    "\n",
    "\n",
    "def lpq_code_image(img: np.ndarray, R: int) -> np.ndarray:\n",
    "    img = img.astype(np.float64, copy=False)\n",
    "    M = 2 * R + 1\n",
    "    x = np.arange(-(M // 2), M // 2 + 1, dtype=np.float64)\n",
    "\n",
    "    w0 = np.ones(M, dtype=np.complex128)\n",
    "    w1 = np.exp(-2j * np.pi * x / M).astype(np.complex128)\n",
    "\n",
    "    def filt_xy(wx, wy):\n",
    "        tmp = convolve1d(img, wx, axis=1, mode=\"reflect\")\n",
    "        out = convolve1d(tmp, wy, axis=0, mode=\"reflect\")\n",
    "        return out\n",
    "\n",
    "    F1 = filt_xy(w1, w0)\n",
    "    F2 = filt_xy(w0, w1)\n",
    "    F3 = filt_xy(w1, w1)\n",
    "    F4 = filt_xy(w1, np.conj(w1))\n",
    "\n",
    "    comps = [np.real(F1), np.imag(F1),\n",
    "             np.real(F2), np.imag(F2),\n",
    "             np.real(F3), np.imag(F3),\n",
    "             np.real(F4), np.imag(F4)]\n",
    "    bits = [(c >= 0).astype(np.uint8) for c in comps]\n",
    "\n",
    "    code = np.zeros(img.shape, dtype=np.uint8)\n",
    "    for i, b in enumerate(bits):\n",
    "        code |= (b << i)\n",
    "    return code\n",
    "\n",
    "\n",
    "def basic_lpq_feature(img: np.ndarray, R: int) -> np.ndarray:\n",
    "    return hist256(lpq_code_image(img, R=R))\n",
    "\n",
    "\n",
    "def mbh_lpq_feature(img: np.ndarray, R: int, b: int) -> np.ndarray:\n",
    "    code = lpq_code_image(img, R=R)\n",
    "    blocks = np.array_split(code, b, axis=0)\n",
    "    return np.concatenate([hist256(bl) for bl in blocks], axis=0)\n",
    "\n",
    "\n",
    "def lbp_feature(img: np.ndarray, P: int, R: int, method: str) -> np.ndarray:\n",
    "    from skimage.feature import local_binary_pattern\n",
    "    lbp = local_binary_pattern(img, P=P, R=R, method=method).astype(np.uint8)\n",
    "    return hist256(lbp)\n",
    "\n",
    "\n",
    "def ldp_code_image(img: np.ndarray, k_top: int = 3) -> np.ndarray:\n",
    "    img = img.astype(np.float32, copy=False)\n",
    "    masks = [\n",
    "        np.array([[5, 5, 5], [-3, 0, -3], [-3, -3, -3]], dtype=np.float32),\n",
    "        np.array([[5, 5, -3], [5, 0, -3], [-3, -3, -3]], dtype=np.float32),\n",
    "        np.array([[5, -3, -3], [5, 0, -3], [5, -3, -3]], dtype=np.float32),\n",
    "        np.array([[-3, -3, -3], [5, 0, -3], [5, 5, -3]], dtype=np.float32),\n",
    "        np.array([[-3, -3, -3], [-3, 0, -3], [5, 5, 5]], dtype=np.float32),\n",
    "        np.array([[-3, -3, -3], [-3, 0, 5], [-3, 5, 5]], dtype=np.float32),\n",
    "        np.array([[-3, -3, 5], [-3, 0, 5], [-3, -3, 5]], dtype=np.float32),\n",
    "        np.array([[-3, 5, 5], [-3, 0, 5], [-3, -3, -3]], dtype=np.float32),\n",
    "    ]\n",
    "    H, W = img.shape\n",
    "    pad = np.pad(img, ((1, 1), (1, 1)), mode=\"reflect\")\n",
    "    resp = np.zeros((8, H, W), dtype=np.float32)\n",
    "    for i, K in enumerate(masks):\n",
    "        r = (\n",
    "            K[0, 0] * pad[0:H, 0:W] + K[0, 1] * pad[0:H, 1:W + 1] + K[0, 2] * pad[0:H, 2:W + 2] +\n",
    "            K[1, 0] * pad[1:H + 1, 0:W] + K[1, 1] * pad[1:H + 1, 1:W + 1] + K[1, 2] * pad[1:H + 1, 2:W + 2] +\n",
    "            K[2, 0] * pad[2:H + 2, 0:W] + K[2, 1] * pad[2:H + 2, 1:W + 1] + K[2, 2] * pad[2:H + 2, 2:W + 2]\n",
    "        )\n",
    "        resp[i] = r\n",
    "\n",
    "    topk = np.argpartition(resp, -k_top, axis=0)[-k_top:]\n",
    "    code = np.zeros((H, W), dtype=np.uint8)\n",
    "    for j in range(k_top):\n",
    "        idx = topk[j]\n",
    "        code |= (1 << idx).astype(np.uint8)\n",
    "    return code\n",
    "\n",
    "\n",
    "def ldp_feature(img: np.ndarray, k_top: int) -> np.ndarray:\n",
    "    return hist256(ldp_code_image(img, k_top=k_top))\n",
    "\n",
    "\n",
    "def vggish_feature_from_waveform(x: np.ndarray, fs: int, device: str) -> np.ndarray:\n",
    "    import torch\n",
    "    import torchaudio\n",
    "\n",
    "    # cố thử pipeline VGGISH của torchaudio\n",
    "    try:\n",
    "        from torchaudio.prototype.pipelines import VGGISH\n",
    "        input_sr = VGGISH.sample_rate\n",
    "        proc = VGGISH.get_input_processor()\n",
    "        model = VGGISH.get_model().to(device)\n",
    "        model.eval()\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(\n",
    "            \"VGGish không chạy được do torchaudio version. \"\n",
    "            \"Cách fix nhanh: vào protocol.yaml -> deep.vggish.enabled=false (dùng VGG16). \"\n",
    "            f\"Chi tiết lỗi: {e}\"\n",
    "        )\n",
    "\n",
    "    wav = torch.from_numpy(x.astype(np.float32))\n",
    "    wav = torchaudio.functional.resample(wav, fs, input_sr)\n",
    "\n",
    "    min_len = int(input_sr * 1.0)\n",
    "    if wav.numel() < min_len:\n",
    "        wav = torch.nn.functional.pad(wav, (0, min_len - wav.numel()))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        ex = proc(wav.to(device))\n",
    "        emb = model(ex)\n",
    "        if emb.ndim == 2:\n",
    "            emb = emb.mean(dim=0)\n",
    "        else:\n",
    "            emb = emb.reshape(-1).mean()\n",
    "    return emb.detach().cpu().numpy().astype(np.float32).ravel()\n",
    "\n",
    "\n",
    "def vgg16_feature_batch(logmels: np.ndarray, layer: str, device: str, batch_size: int = 32) -> np.ndarray:\n",
    "    import torch\n",
    "    import torchvision\n",
    "    import torchvision.transforms as T\n",
    "\n",
    "    weights = torchvision.models.VGG16_Weights.DEFAULT\n",
    "    model = torchvision.models.vgg16(weights=weights).to(device)\n",
    "    model.eval()\n",
    "\n",
    "    norm = T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "\n",
    "    outs = []\n",
    "    N = logmels.shape[0]\n",
    "    for s in range(0, N, batch_size):\n",
    "        x = torch.from_numpy(logmels[s:s + batch_size]).unsqueeze(1)\n",
    "        x = torch.nn.functional.interpolate(x, size=(224, 224), mode=\"bilinear\", align_corners=False)\n",
    "        x = x.repeat(1, 3, 1, 1)\n",
    "        x = norm(x)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            f = model.features(x.to(device))\n",
    "            f = model.avgpool(f)\n",
    "            f = torch.flatten(f, 1)\n",
    "\n",
    "            fc6 = model.classifier[2](model.classifier[1](model.classifier[0](f)))\n",
    "            fc7 = model.classifier[5](model.classifier[4](model.classifier[3](fc6)))\n",
    "            fc8 = model.classifier[6](fc7)\n",
    "\n",
    "        if layer == \"fc6\":\n",
    "            outs.append(fc6.cpu().numpy())\n",
    "        elif layer == \"fc7\":\n",
    "            outs.append(fc7.cpu().numpy())\n",
    "        elif layer == \"fc8\":\n",
    "            outs.append(fc8.cpu().numpy())\n",
    "        else:\n",
    "            raise ValueError(\"layer phải là fc6/fc7/fc8\")\n",
    "\n",
    "    return np.vstack(outs).astype(np.float32)\n",
    "\n",
    "\n",
    "def fit_eda(X: np.ndarray, y: np.ndarray, reg_eps: float, n_components: int) -> np.ndarray:\n",
    "    X = X.astype(np.float64)\n",
    "    y = y.astype(np.int64)\n",
    "    classes = np.unique(y)\n",
    "\n",
    "    mu = X.mean(axis=0, keepdims=True)\n",
    "    Sb = np.zeros((X.shape[1], X.shape[1]), dtype=np.float64)\n",
    "    Sw = np.zeros_like(Sb)\n",
    "\n",
    "    for c in classes:\n",
    "        Xc = X[y == c]\n",
    "        muc = Xc.mean(axis=0, keepdims=True)\n",
    "        d = (muc - mu)\n",
    "        Sb += Xc.shape[0] * (d.T @ d)\n",
    "        X0 = Xc - muc\n",
    "        Sw += (X0.T @ X0)\n",
    "\n",
    "    Sw += reg_eps * np.eye(Sw.shape[0], dtype=np.float64)\n",
    "    Sb = Sb / (np.trace(Sb) + 1e-12)\n",
    "    Sw = Sw / (np.trace(Sw) + 1e-12)\n",
    "\n",
    "    eSb = linalg.expm(Sb)\n",
    "    eSw = linalg.expm(Sw)\n",
    "    vals, vecs = linalg.eigh(eSb, eSw)\n",
    "    idx = np.argsort(vals)[::-1]\n",
    "    W = vecs[:, idx[:n_components]]\n",
    "    return W.astype(np.float64)\n",
    "\n",
    "\n",
    "def cosine_scores(Z: np.ndarray, cents: np.ndarray) -> np.ndarray:\n",
    "    Z = Z.astype(np.float64)\n",
    "    C = cents.astype(np.float64)\n",
    "    Zn = np.linalg.norm(Z, axis=1, keepdims=True) + 1e-12\n",
    "    Cn = np.linalg.norm(C, axis=1, keepdims=True) + 1e-12\n",
    "    return (Z / Zn) @ (C / Cn).T\n",
    "\n",
    "\n",
    "def train_eval_branch(Xtr, ytr, Xva, yva, Xte, yte, pca_dim: int,\n",
    "                      use_pca: bool, use_eda: bool, reg_eps: float, seed: int):\n",
    "    classes = np.unique(ytr)\n",
    "\n",
    "    if use_pca:\n",
    "        pca = PCA(n_components=min(pca_dim, Xtr.shape[1]), random_state=seed)\n",
    "        Xtr2 = pca.fit_transform(Xtr)\n",
    "        Xva2 = pca.transform(Xva)\n",
    "        Xte2 = pca.transform(Xte)\n",
    "    else:\n",
    "        Xtr2, Xva2, Xte2 = Xtr, Xva, Xte\n",
    "\n",
    "    if use_eda:\n",
    "        n_comp = min(len(classes) - 1, Xtr2.shape[1])\n",
    "        W = fit_eda(Xtr2, ytr, reg_eps=reg_eps, n_components=n_comp)\n",
    "        Ztr = Xtr2 @ W\n",
    "        Zva = Xva2 @ W\n",
    "        Zte = Xte2 @ W\n",
    "    else:\n",
    "        Ztr, Zva, Zte = Xtr2, Xva2, Xte2\n",
    "\n",
    "    cents = np.vstack([Ztr[ytr == c].mean(axis=0) for c in classes])\n",
    "    s_va = cosine_scores(Zva, cents)\n",
    "    s_te = cosine_scores(Zte, cents)\n",
    "\n",
    "    yhat_va = classes[np.argmax(s_va, axis=1)]\n",
    "    yhat_te = classes[np.argmax(s_te, axis=1)]\n",
    "\n",
    "    return {\n",
    "        \"scores_val\": s_va,\n",
    "        \"scores_test\": s_te,\n",
    "        \"yhat_val\": yhat_va,\n",
    "        \"yhat_test\": yhat_te,\n",
    "        \"acc_val\": float(accuracy_score(yva, yhat_va)),\n",
    "        \"acc_test\": float(accuracy_score(yte, yhat_te)),\n",
    "        \"f1_val\": float(f1_score(yva, yhat_va, average=\"macro\")),\n",
    "        \"f1_test\": float(f1_score(yte, yhat_te, average=\"macro\")),\n",
    "    }\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class SplitData:\n",
    "    X_train: np.ndarray\n",
    "    y_train: np.ndarray\n",
    "    X_val: np.ndarray\n",
    "    y_val: np.ndarray\n",
    "    X_test: np.ndarray\n",
    "    y_test: np.ndarray\n",
    "    class_names: List[str]\n",
    "\n",
    "\n",
    "def build_cwru_splits(cfg: dict, project_root: str) -> SplitData:\n",
    "    root = os.path.join(project_root, cfg[\"data\"][\"cwru_root\"])\n",
    "    fs = int(cfg[\"data\"][\"fs_hz\"])\n",
    "    seg_len = int(cfg[\"data\"][\"sample_len\"])\n",
    "    n_seg = int(cfg[\"data\"][\"samples_per_class\"])\n",
    "\n",
    "    n_tr = int(cfg[\"data\"][\"split\"][\"train_after_val_per_class\"])\n",
    "    n_val = int(cfg[\"data\"][\"split\"][\"val_per_class\"])\n",
    "    n_te = int(cfg[\"data\"][\"split\"][\"test_per_class\"])\n",
    "\n",
    "    classes_map = cfg[\"data\"][\"classes_10\"]\n",
    "    class_names = list(classes_map.keys())\n",
    "\n",
    "    Xtr, ytr, Xva, yva, Xte, yte = [], [], [], [], [], []\n",
    "\n",
    "    for ci, cname in enumerate(class_names):\n",
    "        fpath = os.path.join(root, classes_map[cname])\n",
    "        mat = loadmat(fpath)\n",
    "        sig = find_1d_signal(mat)\n",
    "\n",
    "        segs = segment_waveform(sig, seg_len=seg_len, n_seg=n_seg)\n",
    "        segs = np.stack([normalize_paper(s) for s in segs], axis=0)\n",
    "\n",
    "        Xtr.append(segs[:n_tr]);                 ytr.append(np.full(n_tr, ci, dtype=np.int64))\n",
    "        Xva.append(segs[n_tr:n_tr+n_val]);       yva.append(np.full(n_val, ci, dtype=np.int64))\n",
    "        Xte.append(segs[n_tr+n_val:n_tr+n_val+n_te]); yte.append(np.full(n_te, ci, dtype=np.int64))\n",
    "\n",
    "    return SplitData(\n",
    "        X_train=np.concatenate(Xtr, axis=0),\n",
    "        y_train=np.concatenate(ytr, axis=0),\n",
    "        X_val=np.concatenate(Xva, axis=0),\n",
    "        y_val=np.concatenate(yva, axis=0),\n",
    "        X_test=np.concatenate(Xte, axis=0),\n",
    "        y_test=np.concatenate(yte, axis=0),\n",
    "        class_names=class_names,\n",
    "    )\n",
    "\n",
    "\n",
    "def run_phase9(protocol_path: str = \"original_paper/protocol.yaml\") -> None:\n",
    "    project_root = os.getcwd()\n",
    "    with open(protocol_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        cfg = yaml.safe_load(f)\n",
    "\n",
    "    seed = int(cfg[\"project\"][\"seed\"])\n",
    "    set_seed(seed)\n",
    "\n",
    "    out_dir = cfg[\"output\"][\"results_dir\"]\n",
    "    cache_dir = cfg[\"output\"][\"cache_dir\"]\n",
    "    ensure_dir(out_dir)\n",
    "    ensure_dir(cache_dir)\n",
    "    ensure_dir(os.path.join(out_dir, \"audit\"))\n",
    "\n",
    "    device = \"cpu\"\n",
    "    try:\n",
    "        import torch\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    split = build_cwru_splits(cfg, project_root)\n",
    "    fs = int(cfg[\"data\"][\"fs_hz\"])\n",
    "\n",
    "    sp = cfg[\"preprocess\"][\"spectrogram\"]\n",
    "    def npy(name): return os.path.join(cache_dir, name)\n",
    "\n",
    "    if not os.path.isfile(npy(\"logmel_train.npy\")):\n",
    "        print(\"[Phase9] Extract log-mel (paper-like)...\")\n",
    "        logmel_train = np.stack([logmel_paper_like(x, fs, **sp) for x in tqdm(split.X_train)], axis=0)\n",
    "        logmel_val   = np.stack([logmel_paper_like(x, fs, **sp) for x in tqdm(split.X_val)], axis=0)\n",
    "        logmel_test  = np.stack([logmel_paper_like(x, fs, **sp) for x in tqdm(split.X_test)], axis=0)\n",
    "        np.save(npy(\"logmel_train.npy\"), logmel_train)\n",
    "        np.save(npy(\"logmel_val.npy\"), logmel_val)\n",
    "        np.save(npy(\"logmel_test.npy\"), logmel_test)\n",
    "    else:\n",
    "        logmel_train = np.load(npy(\"logmel_train.npy\"))\n",
    "        logmel_val   = np.load(npy(\"logmel_val.npy\"))\n",
    "        logmel_test  = np.load(npy(\"logmel_test.npy\"))\n",
    "\n",
    "    plt.figure(figsize=(5,3))\n",
    "    plt.imshow(logmel_train[0].T, aspect=\"auto\", origin=\"lower\")\n",
    "    plt.title(\"Audit log-mel (paper-like)\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(out_dir, \"audit\", \"logmel_example.png\"), dpi=200)\n",
    "    plt.close()\n",
    "\n",
    "    use_pca = bool(cfg[\"reduce_and_classify\"][\"pca\"][\"enabled\"])\n",
    "    pca_dim = int(cfg[\"reduce_and_classify\"][\"pca\"][\"n_components\"])\n",
    "    use_eda = bool(cfg[\"reduce_and_classify\"][\"eda\"][\"enabled\"])\n",
    "    reg_eps = float(cfg[\"reduce_and_classify\"][\"eda\"][\"reg_eps\"])\n",
    "\n",
    "    rows = []\n",
    "    sh = cfg[\"features\"][\"shallow\"]\n",
    "\n",
    "    if sh[\"lbp\"][\"enabled\"]:\n",
    "        P = int(sh[\"lbp\"][\"P\"]); R = int(sh[\"lbp\"][\"R\"]); method = str(sh[\"lbp\"][\"method\"])\n",
    "        print(\"[Phase9] LBP...\")\n",
    "        Xtr = np.stack([lbp_feature(im, P, R, method) for im in tqdm(logmel_train)], axis=0)\n",
    "        Xva = np.stack([lbp_feature(im, P, R, method) for im in tqdm(logmel_val)], axis=0)\n",
    "        Xte = np.stack([lbp_feature(im, P, R, method) for im in tqdm(logmel_test)], axis=0)\n",
    "        res = train_eval_branch(Xtr, split.y_train, Xva, split.y_val, Xte, split.y_test, pca_dim, use_pca, use_eda, reg_eps, seed)\n",
    "        rows.append({\"model\":\"LBP\", **{k:res[k] for k in [\"acc_val\",\"acc_test\",\"f1_val\",\"f1_test\"]}})\n",
    "\n",
    "    if sh[\"ldp\"][\"enabled\"]:\n",
    "        k_top = int(sh[\"ldp\"][\"k_top\"])\n",
    "        print(\"[Phase9] LDP...\")\n",
    "        Xtr = np.stack([ldp_feature(im, k_top) for im in tqdm(logmel_train)], axis=0)\n",
    "        Xva = np.stack([ldp_feature(im, k_top) for im in tqdm(logmel_val)], axis=0)\n",
    "        Xte = np.stack([ldp_feature(im, k_top) for im in tqdm(logmel_test)], axis=0)\n",
    "        res = train_eval_branch(Xtr, split.y_train, Xva, split.y_val, Xte, split.y_test, pca_dim, use_pca, use_eda, reg_eps, seed)\n",
    "        rows.append({\"model\":\"LDP\", **{k:res[k] for k in [\"acc_val\",\"acc_test\",\"f1_val\",\"f1_test\"]}})\n",
    "\n",
    "    if sh[\"lpq\"][\"enabled\"]:\n",
    "        for Rlpq in sh[\"lpq\"][\"R_grid\"]:\n",
    "            Rlpq = int(Rlpq)\n",
    "            print(f\"[Phase9] LPQ R={Rlpq}...\")\n",
    "            Xtr = np.stack([basic_lpq_feature(im, Rlpq) for im in tqdm(logmel_train)], axis=0)\n",
    "            Xva = np.stack([basic_lpq_feature(im, Rlpq) for im in tqdm(logmel_val)], axis=0)\n",
    "            Xte = np.stack([basic_lpq_feature(im, Rlpq) for im in tqdm(logmel_test)], axis=0)\n",
    "            res = train_eval_branch(Xtr, split.y_train, Xva, split.y_val, Xte, split.y_test, pca_dim, use_pca, use_eda, reg_eps, seed)\n",
    "            rows.append({\"model\":f\"LPQ_R{Rlpq}\", **{k:res[k] for k in [\"acc_val\",\"acc_test\",\"f1_val\",\"f1_test\"]}})\n",
    "\n",
    "    mbh = sh[\"mbh_lpq\"]\n",
    "    Rm = int(mbh[\"R\"])\n",
    "    b_grid = [int(x) for x in mbh[\"b_grid\"]]\n",
    "\n",
    "    best_b, best_val = None, -1.0\n",
    "    best_mbh_val_scores, best_mbh_test_scores = None, None\n",
    "    b_rows = []\n",
    "\n",
    "    for b in b_grid:\n",
    "        print(f\"[Phase9] MBH-LPQ R={Rm}, b={b}...\")\n",
    "        Xtr = np.stack([mbh_lpq_feature(im, Rm, b) for im in tqdm(logmel_train)], axis=0)\n",
    "        Xva = np.stack([mbh_lpq_feature(im, Rm, b) for im in tqdm(logmel_val)], axis=0)\n",
    "        Xte = np.stack([mbh_lpq_feature(im, Rm, b) for im in tqdm(logmel_test)], axis=0)\n",
    "        res = train_eval_branch(Xtr, split.y_train, Xva, split.y_val, Xte, split.y_test, pca_dim, use_pca, use_eda, reg_eps, seed)\n",
    "        b_rows.append({\"b\":b, \"acc_val\":res[\"acc_val\"], \"acc_test\":res[\"acc_test\"]})\n",
    "        if res[\"acc_val\"] > best_val:\n",
    "            best_val = res[\"acc_val\"]\n",
    "            best_b = b\n",
    "            best_mbh_val_scores = res[\"scores_val\"]\n",
    "            best_mbh_test_scores = res[\"scores_test\"]\n",
    "\n",
    "    np.savetxt(os.path.join(out_dir, \"b_sweep.csv\"),\n",
    "               np.array([[r[\"b\"], r[\"acc_val\"], r[\"acc_test\"]] for r in b_rows]),\n",
    "               delimiter=\",\", header=\"b,acc_val,acc_test\", comments=\"\")\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot([r[\"b\"] for r in b_rows], [r[\"acc_val\"] for r in b_rows], marker=\"o\", label=\"val\")\n",
    "    plt.plot([r[\"b\"] for r in b_rows], [r[\"acc_test\"] for r in b_rows], marker=\"o\", label=\"test\")\n",
    "    plt.xlabel(\"b (sub-blocks)\"); plt.ylabel(\"Accuracy\"); plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(out_dir, \"b_sweep_plot.png\"), dpi=200)\n",
    "    plt.close()\n",
    "\n",
    "    deep = cfg[\"features\"][\"deep\"]\n",
    "    deep_scores_val = None\n",
    "    deep_scores_test = None\n",
    "    deep_name = None\n",
    "\n",
    "    if deep[\"vggish\"][\"enabled\"]:\n",
    "        print(\"[Phase9] VGGish...\")\n",
    "        cache = os.path.join(cache_dir, \"vggish_train.npy\")\n",
    "        if not os.path.isfile(cache):\n",
    "            Xtr = np.stack([vggish_feature_from_waveform(x, fs, device) for x in tqdm(split.X_train)], axis=0)\n",
    "            Xva = np.stack([vggish_feature_from_waveform(x, fs, device) for x in tqdm(split.X_val)], axis=0)\n",
    "            Xte = np.stack([vggish_feature_from_waveform(x, fs, device) for x in tqdm(split.X_test)], axis=0)\n",
    "            np.save(os.path.join(cache_dir, \"vggish_train.npy\"), Xtr)\n",
    "            np.save(os.path.join(cache_dir, \"vggish_val.npy\"), Xva)\n",
    "            np.save(os.path.join(cache_dir, \"vggish_test.npy\"), Xte)\n",
    "        else:\n",
    "            Xtr = np.load(os.path.join(cache_dir, \"vggish_train.npy\"))\n",
    "            Xva = np.load(os.path.join(cache_dir, \"vggish_val.npy\"))\n",
    "            Xte = np.load(os.path.join(cache_dir, \"vggish_test.npy\"))\n",
    "\n",
    "        res = train_eval_branch(Xtr, split.y_train, Xva, split.y_val, Xte, split.y_test, pca_dim, use_pca, use_eda, reg_eps, seed)\n",
    "        rows.append({\"model\":\"VGGish\", **{k:res[k] for k in [\"acc_val\",\"acc_test\",\"f1_val\",\"f1_test\"]}})\n",
    "        deep_scores_val, deep_scores_test = res[\"scores_val\"], res[\"scores_test\"]\n",
    "        deep_name = \"VGGish\"\n",
    "\n",
    "    if deep[\"vgg16\"][\"enabled\"]:\n",
    "        layer = str(deep[\"vgg16\"][\"layer\"])\n",
    "        print(f\"[Phase9] VGG16_{layer}...\")\n",
    "        cache = os.path.join(cache_dir, f\"vgg16_{layer}_train.npy\")\n",
    "        if not os.path.isfile(cache):\n",
    "            Xtr = vgg16_feature_batch(logmel_train, layer=layer, device=device, batch_size=32)\n",
    "            Xva = vgg16_feature_batch(logmel_val, layer=layer, device=device, batch_size=32)\n",
    "            Xte = vgg16_feature_batch(logmel_test, layer=layer, device=device, batch_size=32)\n",
    "            np.save(os.path.join(cache_dir, f\"vgg16_{layer}_train.npy\"), Xtr)\n",
    "            np.save(os.path.join(cache_dir, f\"vgg16_{layer}_val.npy\"), Xva)\n",
    "            np.save(os.path.join(cache_dir, f\"vgg16_{layer}_test.npy\"), Xte)\n",
    "        else:\n",
    "            Xtr = np.load(os.path.join(cache_dir, f\"vgg16_{layer}_train.npy\"))\n",
    "            Xva = np.load(os.path.join(cache_dir, f\"vgg16_{layer}_val.npy\"))\n",
    "            Xte = np.load(os.path.join(cache_dir, f\"vgg16_{layer}_test.npy\"))\n",
    "\n",
    "        res = train_eval_branch(Xtr, split.y_train, Xva, split.y_val, Xte, split.y_test, pca_dim, use_pca, use_eda, reg_eps, seed)\n",
    "        rows.append({\"model\":f\"VGG16_{layer}\", **{k:res[k] for k in [\"acc_val\",\"acc_test\",\"f1_val\",\"f1_test\"]}})\n",
    "        if deep_scores_val is None:\n",
    "            deep_scores_val, deep_scores_test = res[\"scores_val\"], res[\"scores_test\"]\n",
    "            deep_name = f\"VGG16_{layer}\"\n",
    "\n",
    "    if deep_scores_val is None:\n",
    "        raise RuntimeError(\"Không có deep branch nào chạy được (VGGish/VGG16).\")\n",
    "\n",
    "    alpha_grid = [float(a) for a in cfg[\"fusion\"][\"ws_alpha_grid\"]]\n",
    "    best_alpha, best_ws_val = None, -1.0\n",
    "    best_pred_test = None\n",
    "    classes = np.unique(split.y_train)\n",
    "\n",
    "    ws_rows = []\n",
    "    for a in alpha_grid:\n",
    "        s_val = (1.0 - a) * best_mbh_val_scores + a * deep_scores_val\n",
    "        s_te  = (1.0 - a) * best_mbh_test_scores + a * deep_scores_test\n",
    "        yhat_val = classes[np.argmax(s_val, axis=1)]\n",
    "        yhat_te  = classes[np.argmax(s_te, axis=1)]\n",
    "        acc_val = float(accuracy_score(split.y_val, yhat_val))\n",
    "        acc_te = float(accuracy_score(split.y_test, yhat_te))\n",
    "        ws_rows.append({\"alpha\":a, \"acc_val\":acc_val, \"acc_test\":acc_te})\n",
    "        if acc_val > best_ws_val:\n",
    "            best_ws_val = acc_val\n",
    "            best_alpha = a\n",
    "            best_pred_test = yhat_te\n",
    "\n",
    "    np.savetxt(os.path.join(out_dir, \"ws_sweep.csv\"),\n",
    "               np.array([[r[\"alpha\"], r[\"acc_val\"], r[\"acc_test\"]] for r in ws_rows]),\n",
    "               delimiter=\",\", header=\"alpha,acc_val,acc_test\", comments=\"\")\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot([r[\"alpha\"] for r in ws_rows], [r[\"acc_val\"] for r in ws_rows], marker=\"o\", label=\"val\")\n",
    "    plt.plot([r[\"alpha\"] for r in ws_rows], [r[\"acc_test\"] for r in ws_rows], marker=\"o\", label=\"test\")\n",
    "    plt.xlabel(\"alpha (weight deep)\"); plt.ylabel(\"Accuracy\"); plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(out_dir, \"ws_sweep_plot.png\"), dpi=200)\n",
    "    plt.close()\n",
    "\n",
    "    cm = confusion_matrix(split.y_test, best_pred_test, labels=classes)\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.imshow(cm, interpolation=\"nearest\")\n",
    "    plt.title(f\"Confusion Matrix (alpha={best_alpha:.2f}, deep={deep_name}, b={best_b})\")\n",
    "    plt.colorbar()\n",
    "    tick = np.arange(len(split.class_names))\n",
    "    plt.xticks(tick, split.class_names, rotation=45, ha=\"right\")\n",
    "    plt.yticks(tick, split.class_names)\n",
    "    plt.tight_layout()\n",
    "    plt.xlabel(\"Pred\"); plt.ylabel(\"True\")\n",
    "    plt.savefig(os.path.join(out_dir, \"cm_paper_core.png\"), dpi=200)\n",
    "    plt.close()\n",
    "\n",
    "    import pandas as pd\n",
    "    df = pd.DataFrame(rows)\n",
    "    df.to_csv(os.path.join(out_dir, \"paper_core_metrics.csv\"), index=False)\n",
    "\n",
    "    with open(os.path.join(out_dir, \"best.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump({\"best_b\": int(best_b), \"best_alpha\": float(best_alpha), \"deep_branch\": deep_name}, f, indent=2)\n",
    "\n",
    "    tex = os.path.join(out_dir, \"paper_core_table.tex\")\n",
    "    with open(tex, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"\\\\begin{table}[t]\\\\centering\\\\small\\n\")\n",
    "        f.write(\"\\\\begin{tabular}{lcc}\\\\toprule\\n\")\n",
    "        f.write(\"Model & Acc(val) & Acc(test)\\\\\\\\ \\\\midrule\\n\")\n",
    "        for _, r in df.iterrows():\n",
    "            av = r.get(\"acc_val\", None); at = r.get(\"acc_test\", None)\n",
    "            avs = \"-\" if av is None or (isinstance(av, float) and math.isnan(av)) else f\"{av*100:.2f}\"\n",
    "            ats = \"-\" if at is None or (isinstance(at, float) and math.isnan(at)) else f\"{at*100:.2f}\"\n",
    "            f.write(f\"{r['model']} & {avs} & {ats}\\\\\\\\\\n\")\n",
    "        f.write(\"\\\\bottomrule\\\\end{tabular}\\n\")\n",
    "        f.write(\"\\\\caption{Phase 9 reproduction summary.}\\\\end{table}\\n\")\n",
    "\n",
    "    print(\"[Phase9] DONE ->\", out_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c52931",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile original_paper/method_sciRep_like.py\n",
    "from src.phase9_runner import run_phase9\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_phase9(\"original_paper/protocol.yaml\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a8e6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run original_paper/method_sciRep_like.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca86dab8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
