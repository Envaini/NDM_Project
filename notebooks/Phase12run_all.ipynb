{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a188216d-96d4-4a93-bf21-7f7bf2d16be1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE: created files:\n",
      "- C:\\Users\\ADMIN\\Desktop\\NDM_Project\\experiments\\run_all.py\n",
      "- C:\\Users\\ADMIN\\Desktop\\NDM_Project\\configs\\phase12_run_all.json\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import json, textwrap\n",
    "\n",
    "ROOT = Path(r\"C:\\Users\\ADMIN\\Desktop\\NDM_Project\")  # sửa nếu project bạn nằm chỗ khác\n",
    "(ROOT / \"experiments\").mkdir(parents=True, exist_ok=True)\n",
    "(ROOT / \"configs\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# 1) __init__.py\n",
    "(ROOT / \"experiments\" / \"__init__.py\").write_text(\"\", encoding=\"utf-8\")\n",
    "\n",
    "# 2) configs/phase12_run_all.json\n",
    "cfg = {\n",
    "  \"seed\": 42,\n",
    "\n",
    "  \"cwru_root\": \"data/raw/CWRU\",\n",
    "  \"out_root\": \"results/run_all\",\n",
    "\n",
    "  \"paper_core\": {\n",
    "    \"load\": 0,\n",
    "    \"n_per_class\": 120,\n",
    "    \"seg_len\": 4800,\n",
    "    \"sr\": 48000,\n",
    "    \"test_size\": 0.2,\n",
    "    \"val_size\": 0.2\n",
    "  },\n",
    "\n",
    "  \"lpq_R\": 7,\n",
    "  \"b_list\": [1, 2, 4, 6, 8, 10, 12],\n",
    "  \"alpha_list\": [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9],\n",
    "\n",
    "  \"use_vggish\": True,\n",
    "\n",
    "  \"run_phase10\": False,\n",
    "  \"phase10_dir\": \"results/phase10\",\n",
    "\n",
    "  \"run_phase11\": True,\n",
    "  \"phase11_out_dir\": \"results/phase11\"\n",
    "}\n",
    "(ROOT / \"configs\" / \"phase12_run_all.json\").write_text(json.dumps(cfg, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "# 3) experiments/run_all.py\n",
    "run_all_code = r'''\n",
    "import os, re, json, math, hashlib, subprocess\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io as sio\n",
    "import scipy.signal as sps\n",
    "import scipy.linalg as la\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# Utils\n",
    "# --------------------------\n",
    "def seed_everything(seed: int = 42):\n",
    "    import random\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "def ensure_dir(p: Path):\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "    return p\n",
    "\n",
    "def l2norm(X: np.ndarray, eps: float = 1e-12):\n",
    "    n = np.linalg.norm(X, axis=1, keepdims=True)\n",
    "    return X / (n + eps)\n",
    "\n",
    "def sha256_bytes(b: bytes) -> str:\n",
    "    return hashlib.sha256(b).hexdigest()\n",
    "\n",
    "def sha256_file(p: Path) -> str:\n",
    "    return sha256_bytes(p.read_bytes())\n",
    "\n",
    "def safe_relpath(p: Path, root: Path):\n",
    "    try:\n",
    "        return str(p.resolve().relative_to(root.resolve())).replace(\"\\\\\", \"/\")\n",
    "    except Exception:\n",
    "        return str(p.resolve()).replace(\"\\\\\", \"/\")\n",
    "\n",
    "def get_git_commit(root: Path):\n",
    "    try:\n",
    "        out = subprocess.check_output([\"git\", \"rev-parse\", \"HEAD\"], cwd=str(root), stderr=subprocess.STDOUT)\n",
    "        return out.decode(\"utf-8\").strip()\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def get_versions(pkgs):\n",
    "    import importlib\n",
    "    try:\n",
    "        from importlib import metadata as importlib_metadata\n",
    "    except Exception:\n",
    "        import importlib_metadata\n",
    "\n",
    "    vers = {}\n",
    "    for p in pkgs:\n",
    "        try:\n",
    "            if p == \"python\":\n",
    "                import sys\n",
    "                vers[p] = sys.version.split()[0]\n",
    "                continue\n",
    "            vers[p] = importlib_metadata.version(p)\n",
    "        except Exception:\n",
    "            try:\n",
    "                mod = importlib.import_module(p)\n",
    "                vers[p] = getattr(mod, \"__version__\", None)\n",
    "            except Exception:\n",
    "                vers[p] = None\n",
    "    return vers\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# CWRU parsing\n",
    "# --------------------------\n",
    "def parse_load_from_name(name: str):\n",
    "    m = re.search(r\"_(\\d+)(?:\\.mat)?$\", name)\n",
    "    return int(m.group(1)) if m else None\n",
    "\n",
    "def parse_label_from_name(name: str):\n",
    "    base = name.replace(\".mat\", \"\")\n",
    "    if base.startswith(\"Normal\"):\n",
    "        return \"H\"\n",
    "    if base.startswith(\"B007\"): return \"B007\"\n",
    "    if base.startswith(\"B014\"): return \"B014\"\n",
    "    if base.startswith(\"B021\"): return \"B021\"\n",
    "    if base.startswith(\"IR007\"): return \"IR007\"\n",
    "    if base.startswith(\"IR014\"): return \"IR014\"\n",
    "    if base.startswith(\"IR021\"): return \"IR021\"\n",
    "    # gom OR theo size, bỏ qua @3/@6/@12\n",
    "    if base.startswith(\"OR007\"): return \"OR007\"\n",
    "    if base.startswith(\"OR014\"): return \"OR014\"\n",
    "    if base.startswith(\"OR021\"): return \"OR021\"\n",
    "    return None\n",
    "\n",
    "def load_cwru_de_time(mat_path: Path):\n",
    "    d = sio.loadmat(mat_path, squeeze_me=True)\n",
    "    cand = None\n",
    "    for k in d.keys():\n",
    "        if k.endswith(\"_DE_time\"):\n",
    "            cand = k\n",
    "            break\n",
    "    if cand is None:\n",
    "        raise KeyError(f\"Không tìm thấy *_DE_time trong {mat_path.name}. Keys: {list(d.keys())[:20]}\")\n",
    "    x = d[cand].astype(np.float32)\n",
    "    x = np.ravel(x)\n",
    "    return x\n",
    "\n",
    "def sample_segments(x: np.ndarray, seg_len: int, n_seg: int, rng: np.random.Generator):\n",
    "    if len(x) <= seg_len:\n",
    "        pad = seg_len - len(x) + 1\n",
    "        x = np.pad(x, (0, pad), mode=\"wrap\")\n",
    "    max_start = len(x) - seg_len\n",
    "    starts = rng.integers(0, max_start + 1, size=n_seg)\n",
    "    segs = np.stack([x[s:s+seg_len] for s in starts], axis=0)\n",
    "    return segs\n",
    "\n",
    "def scan_metadata(cwru_root: Path):\n",
    "    rows = []\n",
    "    fault_dir = cwru_root / \"48k_drive_end_fault\"\n",
    "    normal_dir = cwru_root / \"normal_baseline\"\n",
    "\n",
    "    for p in list(fault_dir.glob(\"*.mat\")) + list(normal_dir.glob(\"*.mat\")):\n",
    "        name = p.name\n",
    "        y = parse_label_from_name(name)\n",
    "        ld = parse_load_from_name(name)\n",
    "        if y is None or ld is None:\n",
    "            continue\n",
    "        rows.append({\"path\": str(p), \"name\": name, \"label\": y, \"load\": ld})\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    if df.empty:\n",
    "        raise RuntimeError(\"Không scan được file *.mat theo format *_<load>.mat. Kiểm tra tên file và folder.\")\n",
    "    return df\n",
    "\n",
    "def build_samples_single_load(df: pd.DataFrame, load: int, n_per_class: int, seg_len: int, seed: int):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    sub = df[df[\"load\"] == load].copy()\n",
    "    labels = sorted(sub[\"label\"].unique().tolist())\n",
    "\n",
    "    X_wave = []\n",
    "    y = []\n",
    "\n",
    "    for lab in labels:\n",
    "        files = sub[sub[\"label\"] == lab][\"path\"].tolist()\n",
    "        if len(files) == 0:\n",
    "            continue\n",
    "        need = n_per_class\n",
    "        per_file = max(1, math.ceil(need / len(files)))\n",
    "        got = 0\n",
    "        for fp in files:\n",
    "            x = load_cwru_de_time(Path(fp))\n",
    "            segs = sample_segments(x, seg_len=seg_len, n_seg=per_file, rng=rng)\n",
    "            take = min(segs.shape[0], need - got)\n",
    "            X_wave.append(segs[:take])\n",
    "            y += [lab] * take\n",
    "            got += take\n",
    "            if got >= need:\n",
    "                break\n",
    "\n",
    "    X_wave = np.concatenate(X_wave, axis=0).astype(np.float32)\n",
    "    y = np.array(y)\n",
    "    idx = rng.permutation(len(y))\n",
    "    return X_wave[idx], y[idx]\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# Log-mel (96,64)\n",
    "# --------------------------\n",
    "def logmel_96x64(wave: np.ndarray, sr: int = 48000, n_fft: int = 1024, hop: int = 256, n_mels: int = 64, n_frames: int = 96):\n",
    "    try:\n",
    "        import librosa\n",
    "    except Exception as e:\n",
    "        raise ImportError(\"Thiếu librosa. Cài: pip install librosa\") from e\n",
    "\n",
    "    w = wave.astype(np.float32)\n",
    "    w = w - w.mean()\n",
    "    w = w / (np.max(np.abs(w)) + 1e-9)\n",
    "\n",
    "    S = librosa.feature.melspectrogram(y=w, sr=sr, n_fft=n_fft, hop_length=hop, n_mels=n_mels, power=2.0)\n",
    "    logS = librosa.power_to_db(S, ref=np.max)\n",
    "    img = logS.T\n",
    "\n",
    "    T = img.shape[0]\n",
    "    if T >= n_frames:\n",
    "        img = img[:n_frames, :]\n",
    "    else:\n",
    "        pad = n_frames - T\n",
    "        img = np.pad(img, ((0, pad), (0, 0)), mode=\"edge\")\n",
    "\n",
    "    mn, mx = img.min(), img.max()\n",
    "    img = (img - mn) / (mx - mn + 1e-9)\n",
    "    return img.astype(np.float32)\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# LPQ + MBH-LPQ\n",
    "# --------------------------\n",
    "def lpq_codes(img: np.ndarray, R: int = 7):\n",
    "    a = 1.0 / (2*R + 1)\n",
    "    x = np.arange(-R, R+1)\n",
    "    y = np.arange(-R, R+1)\n",
    "    X, Y = np.meshgrid(x, y, indexing=\"xy\")\n",
    "\n",
    "    w1 = np.exp(-2j*np.pi*a*X)\n",
    "    w2 = np.exp(-2j*np.pi*a*Y)\n",
    "\n",
    "    f1 = w1\n",
    "    f2 = w2\n",
    "    f3 = w1*w2\n",
    "    f4 = w1*np.conj(w2)\n",
    "\n",
    "    def conv_complex(f):\n",
    "        re = sps.convolve2d(img, np.real(f), mode=\"same\", boundary=\"symm\")\n",
    "        im = sps.convolve2d(img, np.imag(f), mode=\"same\", boundary=\"symm\")\n",
    "        return re, im\n",
    "\n",
    "    re1, im1 = conv_complex(f1)\n",
    "    re2, im2 = conv_complex(f2)\n",
    "    re3, im3 = conv_complex(f3)\n",
    "    re4, im4 = conv_complex(f4)\n",
    "\n",
    "    bits = [\n",
    "        (re1 > 0), (im1 > 0),\n",
    "        (re2 > 0), (im2 > 0),\n",
    "        (re3 > 0), (im3 > 0),\n",
    "        (re4 > 0), (im4 > 0),\n",
    "    ]\n",
    "    code = np.zeros(img.shape, dtype=np.uint8)\n",
    "    for i, b in enumerate(bits):\n",
    "        code |= (b.astype(np.uint8) << i)\n",
    "    return code\n",
    "\n",
    "def best_grid(b: int):\n",
    "    best = (1, b, 10**9)\n",
    "    for gh in range(1, b+1):\n",
    "        if b % gh == 0:\n",
    "            gw = b // gh\n",
    "            score = abs(gh - gw)\n",
    "            if score < best[2]:\n",
    "                best = (gh, gw, score)\n",
    "    return best[0], best[1]\n",
    "\n",
    "def mbh_lpq_feature(code_img: np.ndarray, b: int):\n",
    "    H, W = code_img.shape\n",
    "    gh, gw = best_grid(b)\n",
    "    feats = []\n",
    "    for i in range(gh):\n",
    "        for j in range(gw):\n",
    "            r0 = int(round(i * H / gh))\n",
    "            r1 = int(round((i+1) * H / gh))\n",
    "            c0 = int(round(j * W / gw))\n",
    "            c1 = int(round((j+1) * W / gw))\n",
    "            block = code_img[r0:r1, c0:c1]\n",
    "            hist = np.bincount(block.ravel(), minlength=256).astype(np.float32)\n",
    "            hist = hist / (hist.sum() + 1e-9)\n",
    "            feats.append(hist)\n",
    "    return np.concatenate(feats, axis=0).astype(np.float32)\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# PCA + EDA (stable) + fallback LDA\n",
    "# --------------------------\n",
    "def fit_pca_eda_stable(Xtr: np.ndarray, ytr: np.ndarray, pca_dim: int = 128, out_dim: int = None, reg: float = 1e-6):\n",
    "    scaler = StandardScaler()\n",
    "    Xs = scaler.fit_transform(Xtr)\n",
    "\n",
    "    d = Xs.shape[1]\n",
    "    n = Xs.shape[0]\n",
    "    p = min(pca_dim, d, max(2, n-1))\n",
    "    pca = PCA(n_components=p, random_state=0)\n",
    "    Xp = pca.fit_transform(Xs)\n",
    "\n",
    "    classes = np.unique(ytr)\n",
    "    mu = Xp.mean(axis=0, keepdims=True)\n",
    "\n",
    "    Sw = np.zeros((p, p), dtype=np.float64)\n",
    "    Sb = np.zeros((p, p), dtype=np.float64)\n",
    "\n",
    "    for c in classes:\n",
    "        Xc = Xp[ytr == c]\n",
    "        muc = Xc.mean(axis=0, keepdims=True)\n",
    "        Sw += (Xc - muc).T @ (Xc - muc)\n",
    "        Sb += Xc.shape[0] * (muc - mu).T @ (muc - mu)\n",
    "\n",
    "    Sw = Sw / max(1, (Xp.shape[0] - len(classes)))\n",
    "    Sb = Sb / max(1, len(classes))\n",
    "\n",
    "    # scale by trace to avoid expm overflow\n",
    "    Sw = Sw / (np.trace(Sw) + 1e-9)\n",
    "    Sb = Sb / (np.trace(Sb) + 1e-9)\n",
    "\n",
    "    Sw += reg * np.eye(p)\n",
    "\n",
    "    # try EDA (expm)\n",
    "    try:\n",
    "        A = la.expm(Sb)\n",
    "        B = la.expm(Sw)\n",
    "        A = (A + A.T) / 2.0\n",
    "        B = (B + B.T) / 2.0\n",
    "        if (not np.isfinite(A).all()) or (not np.isfinite(B).all()):\n",
    "            raise ValueError(\"EDA expm produced inf/nan\")\n",
    "        w, V = la.eigh(A, B)\n",
    "    except Exception:\n",
    "        # fallback LDA: solve Sb v = λ Sw v\n",
    "        w, V = la.eigh(Sb, Sw)\n",
    "\n",
    "    idx = np.argsort(w)[::-1]\n",
    "    V = V[:, idx]\n",
    "\n",
    "    if out_dim is None:\n",
    "        out_dim = min(len(classes) - 1, p)\n",
    "        out_dim = max(2, out_dim)\n",
    "    W = V[:, :out_dim].astype(np.float32)\n",
    "\n",
    "    def transform(X: np.ndarray):\n",
    "        Xs2 = scaler.transform(X)\n",
    "        Xp2 = pca.transform(Xs2)\n",
    "        Z = Xp2 @ W\n",
    "        return Z.astype(np.float32)\n",
    "\n",
    "    return transform\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# Cosine-prototype classifier\n",
    "# --------------------------\n",
    "def fit_prototypes(Xtr: np.ndarray, ytr: np.ndarray):\n",
    "    Xn = l2norm(Xtr)\n",
    "    classes = np.unique(ytr)\n",
    "    protos = []\n",
    "    for c in classes:\n",
    "        pc = Xn[ytr == c].mean(axis=0, keepdims=True)\n",
    "        pc = l2norm(pc)\n",
    "        protos.append(pc)\n",
    "    P = np.concatenate(protos, axis=0)\n",
    "    return classes, P\n",
    "\n",
    "def predict_with_protos(X: np.ndarray, classes: np.ndarray, P: np.ndarray):\n",
    "    Xn = l2norm(X)\n",
    "    scores = Xn @ P.T\n",
    "    pred = classes[np.argmax(scores, axis=1)]\n",
    "    return pred, scores\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# VGGish embed\n",
    "# --------------------------\n",
    "def try_init_vggish(sr: int = 48000):\n",
    "    try:\n",
    "        import torch\n",
    "        import torchaudio\n",
    "        from torchaudio.prototype.pipelines import VGGISH\n",
    "        bundle = VGGISH\n",
    "        model = bundle.get_model()\n",
    "        model.eval()\n",
    "        iproc = bundle.get_input_processor()\n",
    "        resampler = torchaudio.transforms.Resample(orig_freq=sr, new_freq=16000)\n",
    "        return True, model, iproc, resampler\n",
    "    except Exception as e:\n",
    "        print(\"[WARN] Không load được torchaudio VGGISH -> chạy shallow-only. Lỗi:\", repr(e))\n",
    "        return False, None, None, None\n",
    "\n",
    "def vggish_embed_batch(Xwave: np.ndarray, model, iproc, resampler):\n",
    "    import torch\n",
    "    feats = []\n",
    "    with torch.no_grad():\n",
    "        for w in Xwave:\n",
    "            t = torch.from_numpy(w).float()\n",
    "            t = t - t.mean()\n",
    "            t = t / (t.abs().max() + 1e-9)\n",
    "\n",
    "            t16 = resampler(t)\n",
    "            if t16.ndim != 1:\n",
    "                t16 = t16.squeeze()\n",
    "            t16 = t16.reshape(-1)\n",
    "\n",
    "            # iproc signature varies\n",
    "            try:\n",
    "                inp = iproc(t16)\n",
    "            except TypeError:\n",
    "                inp = iproc(t16, sample_rate=16000)\n",
    "\n",
    "            out = model(inp)\n",
    "            if out.ndim == 3:\n",
    "                v = out.mean(dim=1).squeeze(0)\n",
    "            else:\n",
    "                v = out.squeeze(0)\n",
    "            feats.append(v.cpu().numpy().astype(np.float32))\n",
    "    return np.stack(feats, axis=0)\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# Paper-core run (Phase12 required artifacts)\n",
    "# --------------------------\n",
    "def extract_shallow_features(Xwave: np.ndarray, sr: int, lpq_R: int, b_list, audit_dir: Path):\n",
    "    feats_b = {b: [] for b in b_list}\n",
    "    audit_saved = False\n",
    "    for w in Xwave:\n",
    "        img = logmel_96x64(w, sr=sr)\n",
    "        if not audit_saved:\n",
    "            plt.figure()\n",
    "            plt.imshow(img.T, aspect=\"auto\", origin=\"lower\")\n",
    "            plt.title(\"Audit log-mel (96x64)\")\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(audit_dir / \"audit_logmel.png\", dpi=200)\n",
    "            plt.close()\n",
    "            audit_saved = True\n",
    "\n",
    "        code = lpq_codes(img, R=lpq_R)\n",
    "        for b in b_list:\n",
    "            feats_b[b].append(mbh_lpq_feature(code, b=b))\n",
    "\n",
    "    for b in b_list:\n",
    "        feats_b[b] = np.stack(feats_b[b], axis=0).astype(np.float32)\n",
    "    return feats_b\n",
    "\n",
    "def save_confusion_matrix(y_true, y_pred, labels, out_png: Path, title: str):\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "    plt.figure(figsize=(7,6))\n",
    "    plt.imshow(cm, aspect=\"auto\")\n",
    "    plt.title(title)\n",
    "    plt.xticks(range(len(labels)), labels, rotation=45, ha=\"right\")\n",
    "    plt.yticks(range(len(labels)), labels)\n",
    "    plt.colorbar()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_png, dpi=200)\n",
    "    plt.close()\n",
    "\n",
    "def run_paper_core(df_meta: pd.DataFrame, cfg: dict, root: Path, run_dir: Path):\n",
    "    seed = int(cfg[\"seed\"])\n",
    "    pc = cfg[\"paper_core\"]\n",
    "    load = int(pc[\"load\"])\n",
    "    n_per_class = int(pc[\"n_per_class\"])\n",
    "    seg_len = int(pc[\"seg_len\"])\n",
    "    sr = int(pc[\"sr\"])\n",
    "    test_size = float(pc[\"test_size\"])\n",
    "    val_size = float(pc[\"val_size\"])\n",
    "\n",
    "    lpq_R = int(cfg[\"lpq_R\"])\n",
    "    b_list = list(cfg[\"b_list\"])\n",
    "    alpha_list = list(cfg[\"alpha_list\"])\n",
    "    use_vggish = bool(cfg.get(\"use_vggish\", True))\n",
    "\n",
    "    seed_everything(seed)\n",
    "    audit_dir = ensure_dir(run_dir / \"audit\")\n",
    "\n",
    "    Xwave, y = build_samples_single_load(df_meta, load=load, n_per_class=n_per_class, seg_len=seg_len, seed=seed)\n",
    "\n",
    "    # split train+val vs test\n",
    "    X_trv, X_te, y_trv, y_te = train_test_split(Xwave, y, test_size=test_size, random_state=seed, stratify=y)\n",
    "    # split train vs val\n",
    "    X_tr, X_va, y_tr, y_va = train_test_split(X_trv, y_trv, test_size=val_size, random_state=seed, stratify=y_trv)\n",
    "\n",
    "    # shallow\n",
    "    print(\"[Phase12] paper-core: extract shallow ...\")\n",
    "    tr_sh = extract_shallow_features(X_tr, sr, lpq_R, b_list, audit_dir)\n",
    "    va_sh = extract_shallow_features(X_va, sr, lpq_R, b_list, audit_dir)\n",
    "    te_sh = extract_shallow_features(X_te, sr, lpq_R, b_list, audit_dir)\n",
    "\n",
    "    # deep\n",
    "    deep_ok = False\n",
    "    Xtr_de = Xva_de = Xte_de = None\n",
    "    model = iproc = resampler = None\n",
    "    if use_vggish:\n",
    "        deep_ok, model, iproc, resampler = try_init_vggish(sr=sr)\n",
    "        if deep_ok:\n",
    "            print(\"[Phase12] paper-core: extract deep (VGGish) ...\")\n",
    "            Xtr_de = vggish_embed_batch(X_tr, model, iproc, resampler)\n",
    "            Xva_de = vggish_embed_batch(X_va, model, iproc, resampler)\n",
    "            Xte_de = vggish_embed_batch(X_te, model, iproc, resampler)\n",
    "\n",
    "    rows_metrics = []\n",
    "    rows_ws = []\n",
    "    rows_ablation = []\n",
    "\n",
    "    # deep-only\n",
    "    if deep_ok:\n",
    "        de_tf = fit_pca_eda_stable(Xtr_de, y_tr, pca_dim=128)\n",
    "        Ztr = de_tf(Xtr_de); Zva = de_tf(Xva_de); Zte = de_tf(Xte_de)\n",
    "        cls, P = fit_prototypes(Ztr, y_tr)\n",
    "        pv, Sv = predict_with_protos(Zva, cls, P)\n",
    "        pt, St = predict_with_protos(Zte, cls, P)\n",
    "        rows_metrics.append({\"model\":\"VGGish\", \"best_b\":None, \"best_alpha\":1.0,\n",
    "                             \"acc_val\":accuracy_score(y_va, pv), \"acc_test\":accuracy_score(y_te, pt),\n",
    "                             \"f1_val\":f1_score(y_va, pv, average=\"macro\"), \"f1_test\":f1_score(y_te, pt, average=\"macro\")})\n",
    "    else:\n",
    "        Sv = St = None\n",
    "        cls = None\n",
    "\n",
    "    # shallow sweep b + fusion sweep alpha\n",
    "    best = {\"best_model\": None, \"best_b\": None, \"best_alpha\": None, \"best_acc_val\": -1.0}\n",
    "\n",
    "    for b in b_list:\n",
    "        sh_tf = fit_pca_eda_stable(tr_sh[b], y_tr, pca_dim=256)\n",
    "        Ztr = sh_tf(tr_sh[b]); Zva = sh_tf(va_sh[b]); Zte = sh_tf(te_sh[b])\n",
    "        cls2, P2 = fit_prototypes(Ztr, y_tr)\n",
    "        pv_sh, Sv_sh = predict_with_protos(Zva, cls2, P2)\n",
    "        pt_sh, St_sh = predict_with_protos(Zte, cls2, P2)\n",
    "\n",
    "        accv = accuracy_score(y_va, pv_sh)\n",
    "        acct = accuracy_score(y_te, pt_sh)\n",
    "\n",
    "        rows_ablation.append({\"b\": int(b), \"acc_val\": float(accv), \"acc_test\": float(acct)})\n",
    "        rows_metrics.append({\"model\":\"MBH-LPQ\", \"best_b\":int(b), \"best_alpha\":0.0,\n",
    "                             \"acc_val\":accv, \"acc_test\":acct,\n",
    "                             \"f1_val\":f1_score(y_va, pv_sh, average=\"macro\"), \"f1_test\":f1_score(y_te, pt_sh, average=\"macro\")})\n",
    "\n",
    "        if accv > best[\"best_acc_val\"]:\n",
    "            best.update({\"best_model\":\"MBH-LPQ\", \"best_b\":int(b), \"best_alpha\":0.0, \"best_acc_val\":float(accv)})\n",
    "\n",
    "        if deep_ok:\n",
    "            # align class order\n",
    "            all_cls = np.array(sorted(set(cls.tolist()) | set(cls2.tolist())))\n",
    "\n",
    "            def align_scores(scores, cls_src):\n",
    "                m = {c:i for i,c in enumerate(cls_src)}\n",
    "                out = np.zeros((scores.shape[0], len(all_cls)), dtype=np.float32)\n",
    "                for j,c in enumerate(all_cls):\n",
    "                    out[:, j] = scores[:, m[c]] if c in m else -1e9\n",
    "                return out\n",
    "\n",
    "            Sv_de = align_scores(Sv, cls)\n",
    "            St_de = align_scores(St, cls)\n",
    "            Sv_sh_al = align_scores(Sv_sh, cls2)\n",
    "            St_sh_al = align_scores(St_sh, cls2)\n",
    "\n",
    "            for alpha in cfg[\"alpha_list\"]:\n",
    "                alpha = float(alpha)\n",
    "                Sv_f = alpha*Sv_de + (1-alpha)*Sv_sh_al\n",
    "                St_f = alpha*St_de + (1-alpha)*St_sh_al\n",
    "\n",
    "                pv_f = all_cls[np.argmax(Sv_f, axis=1)]\n",
    "                pt_f = all_cls[np.argmax(St_f, axis=1)]\n",
    "\n",
    "                accv_f = accuracy_score(y_va, pv_f)\n",
    "                acct_f = accuracy_score(y_te, pt_f)\n",
    "\n",
    "                rows_ws.append({\"b\": int(b), \"alpha\": alpha, \"acc_val\": float(accv_f), \"acc_test\": float(acct_f)})\n",
    "\n",
    "                if accv_f > best[\"best_acc_val\"]:\n",
    "                    best.update({\"best_model\":\"Fusion\", \"best_b\":int(b), \"best_alpha\":float(alpha), \"best_acc_val\":float(accv_f)})\n",
    "\n",
    "    # --- Save required artifacts ---\n",
    "    paper_core_metrics = pd.DataFrame(rows_metrics)\n",
    "    paper_core_metrics.to_csv(run_dir / \"paper_core_metrics.csv\", index=False)\n",
    "\n",
    "    ws_df = pd.DataFrame(rows_ws)\n",
    "    ws_df.to_csv(run_dir / \"ws_sweep.csv\", index=False)\n",
    "\n",
    "    ab_df = pd.DataFrame(rows_ablation).sort_values(\"acc_val\", ascending=False)\n",
    "    ab_df.to_csv(run_dir / \"ablation_summary.csv\", index=False)\n",
    "\n",
    "    # confusion matrix for best\n",
    "    labels = np.array(sorted(set(y_tr.tolist()) | set(y_te.tolist())))\n",
    "    if best[\"best_model\"] == \"Fusion\" and deep_ok:\n",
    "        b = best[\"best_b\"]; alpha = best[\"best_alpha\"]\n",
    "\n",
    "        # rebuild shallow scores\n",
    "        sh_tf = fit_pca_eda_stable(tr_sh[b], y_tr, pca_dim=256)\n",
    "        Ztr_sh = sh_tf(tr_sh[b]); Zte_sh = sh_tf(te_sh[b])\n",
    "        cls2, P2 = fit_prototypes(Ztr_sh, y_tr)\n",
    "        _, St_sh = predict_with_protos(Zte_sh, cls2, P2)\n",
    "\n",
    "        # rebuild deep scores\n",
    "        de_tf = fit_pca_eda_stable(Xtr_de, y_tr, pca_dim=128)\n",
    "        Ztr_de = de_tf(Xtr_de); Zte_de = de_tf(Xte_de)\n",
    "        cls, P = fit_prototypes(Ztr_de, y_tr)\n",
    "        _, St_de = predict_with_protos(Zte_de, cls, P)\n",
    "\n",
    "        all_cls = np.array(sorted(set(cls.tolist()) | set(cls2.tolist())))\n",
    "\n",
    "        def align(scores, cls_src):\n",
    "            m = {c:i for i,c in enumerate(cls_src)}\n",
    "            out = np.zeros((scores.shape[0], len(all_cls)), dtype=np.float32)\n",
    "            for j,c in enumerate(all_cls):\n",
    "                out[:, j] = scores[:, m[c]] if c in m else -1e9\n",
    "            return out\n",
    "\n",
    "        St_f = alpha*align(St_de, cls) + (1-alpha)*align(St_sh, cls2)\n",
    "        y_pred = all_cls[np.argmax(St_f, axis=1)]\n",
    "        cm_path = run_dir / f\"cm_fusion_b{b}_a{alpha:.1f}.png\"\n",
    "        save_confusion_matrix(y_te, y_pred, all_cls, cm_path, f\"Paper-core CM (load={load}) Fusion b={b} a={alpha:.1f}\")\n",
    "    else:\n",
    "        # best shallow\n",
    "        b = best[\"best_b\"]\n",
    "        sh_tf = fit_pca_eda_stable(tr_sh[b], y_tr, pca_dim=256)\n",
    "        Ztr_sh = sh_tf(tr_sh[b]); Zte_sh = sh_tf(te_sh[b])\n",
    "        cls2, P2 = fit_prototypes(Ztr_sh, y_tr)\n",
    "        y_pred, _ = predict_with_protos(Zte_sh, cls2, P2)\n",
    "        cm_path = run_dir / f\"cm_shallow_b{b}.png\"\n",
    "        save_confusion_matrix(y_te, y_pred, labels, cm_path, f\"Paper-core CM (load={load}) MBH-LPQ b={b}\")\n",
    "\n",
    "    # metrics.csv (required): one-line summary + key numbers\n",
    "    best_row = paper_core_metrics.sort_values(\"acc_val\", ascending=False).head(1).iloc[0].to_dict()\n",
    "    metrics = pd.DataFrame([{\n",
    "        \"paper_core_load\": load,\n",
    "        \"best_model\": best[\"best_model\"],\n",
    "        \"best_b\": best[\"best_b\"],\n",
    "        \"best_alpha\": best[\"best_alpha\"],\n",
    "        \"best_acc_val\": best[\"best_acc_val\"],\n",
    "        \"best_acc_test\": float(best_row.get(\"acc_test\", np.nan)),\n",
    "        \"deep_ok\": bool(deep_ok)\n",
    "    }])\n",
    "    metrics.to_csv(run_dir / \"metrics.csv\", index=False)\n",
    "\n",
    "    return {\n",
    "        \"deep_ok\": deep_ok,\n",
    "        \"best\": best,\n",
    "        \"artifacts\": [\n",
    "            run_dir / \"paper_core_metrics.csv\",\n",
    "            run_dir / \"ws_sweep.csv\",\n",
    "            run_dir / \"ablation_summary.csv\",\n",
    "            run_dir / \"metrics.csv\",\n",
    "            cm_path\n",
    "        ]\n",
    "    }\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# Phase11 analysis from Phase10 outputs (optional)\n",
    "# --------------------------\n",
    "def run_phase11_analysis(root: Path, phase10_dir: Path, out_dir: Path):\n",
    "    out_dir = ensure_dir(out_dir)\n",
    "\n",
    "    sum_csv = phase10_dir / \"domain_shift_summary.csv\"\n",
    "    mat_csv = phase10_dir / \"domain_shift_matrix.csv\"\n",
    "    if (not sum_csv.exists()) or (not mat_csv.exists()):\n",
    "        print(\"[Phase12] Phase11 skip: không thấy Phase10 outputs:\", str(sum_csv), str(mat_csv))\n",
    "        return {\"artifacts\": []}\n",
    "\n",
    "    sdf = pd.read_csv(sum_csv)\n",
    "    piv = pd.read_csv(mat_csv, index_col=0)\n",
    "\n",
    "    # heatmap\n",
    "    plt.figure(figsize=(7,6))\n",
    "    plt.imshow(piv.values, aspect=\"auto\")\n",
    "    plt.title(\"Domain shift accuracy (train load A -> test load B)\")\n",
    "    plt.xticks(range(piv.shape[1]), piv.columns.astype(str))\n",
    "    plt.yticks(range(piv.shape[0]), piv.index.astype(str))\n",
    "    plt.xlabel(\"test_load\")\n",
    "    plt.ylabel(\"train_load\")\n",
    "    plt.colorbar()\n",
    "    plt.tight_layout()\n",
    "    heat_png = out_dir / \"heatmap_domain_shift.png\"\n",
    "    plt.savefig(heat_png, dpi=200)\n",
    "    plt.close()\n",
    "\n",
    "    # diag/off stats\n",
    "    M = piv.values.astype(float)\n",
    "    diag = np.diag(M)\n",
    "    mask = ~np.eye(M.shape[0], dtype=bool)\n",
    "    off = M[mask]\n",
    "\n",
    "    stats = {\n",
    "        \"diag_mean\": float(np.mean(diag)),\n",
    "        \"diag_std\": float(np.std(diag)),\n",
    "        \"off_mean\": float(np.mean(off)),\n",
    "        \"off_std\": float(np.std(off)),\n",
    "        \"gap_diag_minus_off\": float(np.mean(diag) - np.mean(off)),\n",
    "        \"min_diag\": float(np.min(diag)),\n",
    "        \"max_diag\": float(np.max(diag)),\n",
    "        \"min_off\": float(np.min(off)),\n",
    "        \"max_off\": float(np.max(off)),\n",
    "    }\n",
    "    stats_df = pd.DataFrame([stats])\n",
    "    stats_csv = out_dir / \"phase11_diag_off_stats.csv\"\n",
    "    stats_df.to_csv(stats_csv, index=False)\n",
    "\n",
    "    return {\"artifacts\": [heat_png, stats_csv]}\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# Main run_all\n",
    "# --------------------------\n",
    "def main():\n",
    "    import argparse\n",
    "    ap = argparse.ArgumentParser()\n",
    "    ap.add_argument(\"--config\", required=True, help=\"Path to config JSON\")\n",
    "    args = ap.parse_args()\n",
    "\n",
    "    # ROOT = parent of experiments/\n",
    "    root = Path(__file__).resolve().parents[1]\n",
    "\n",
    "    cfg_path = (root / args.config).resolve() if not Path(args.config).is_absolute() else Path(args.config)\n",
    "    cfg = json.loads(cfg_path.read_text(encoding=\"utf-8\"))\n",
    "    seed = int(cfg.get(\"seed\", 42))\n",
    "    seed_everything(seed)\n",
    "\n",
    "    # run id\n",
    "    ts = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    cfg_hash = sha256_bytes(cfg_path.read_bytes())[:12]\n",
    "    run_id = f\"{ts}_{cfg_hash}\"\n",
    "\n",
    "    out_root = root / cfg.get(\"out_root\", \"results/run_all\")\n",
    "    run_dir = ensure_dir(out_root / run_id)\n",
    "\n",
    "    # save a copy config inside run folder\n",
    "    (run_dir / \"config_used.json\").write_text(json.dumps(cfg, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "    # scan CWRU\n",
    "    cwru_root = root / cfg[\"cwru_root\"]\n",
    "    df_meta = scan_metadata(cwru_root)\n",
    "\n",
    "    artifacts = []\n",
    "    print(\"[Phase12] RUN paper-core -> required artifacts ...\")\n",
    "    pc_res = run_paper_core(df_meta, cfg, root=root, run_dir=run_dir)\n",
    "    artifacts += pc_res[\"artifacts\"]\n",
    "\n",
    "    # optional Phase11 analysis from Phase10 outputs\n",
    "    if bool(cfg.get(\"run_phase11\", True)):\n",
    "        phase10_dir = root / cfg.get(\"phase10_dir\", \"results/phase10\")\n",
    "        phase11_out = root / cfg.get(\"phase11_out_dir\", \"results/phase11\")\n",
    "        print(\"[Phase12] RUN phase11 analysis (optional) ...\")\n",
    "        p11 = run_phase11_analysis(root=root, phase10_dir=phase10_dir, out_dir=phase11_out)\n",
    "        artifacts += p11[\"artifacts\"]\n",
    "\n",
    "    # manifest\n",
    "    manifest = {\n",
    "        \"timestamp\": datetime.now().isoformat(timespec=\"seconds\"),\n",
    "        \"seed\": seed,\n",
    "        \"config_path\": safe_relpath(cfg_path, root),\n",
    "        \"config_sha256\": sha256_file(cfg_path),\n",
    "        \"run_id\": run_id,\n",
    "        \"git_commit\": get_git_commit(root),\n",
    "        \"versions\": get_versions([\"python\",\"numpy\",\"pandas\",\"scikit-learn\",\"scipy\",\"matplotlib\",\"torch\",\"torchaudio\",\"librosa\"]),\n",
    "        \"artifacts\": [safe_relpath(p, root) for p in artifacts if p is not None and Path(p).exists()]\n",
    "    }\n",
    "\n",
    "    # must be results/manifest.json (per requirement)\n",
    "    results_dir = ensure_dir(root / \"results\")\n",
    "    (results_dir / \"manifest.json\").write_text(json.dumps(manifest, indent=2), encoding=\"utf-8\")\n",
    "    (run_dir / \"manifest.json\").write_text(json.dumps(manifest, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "    print(\"\\n[Phase12] DONE\")\n",
    "    print(\"Run folder:\", str(run_dir.resolve()))\n",
    "    print(\"Manifest:\", str((results_dir / \"manifest.json\").resolve()))\n",
    "    print(\"Required files present?\")\n",
    "    required = [\"metrics.csv\", \"paper_core_metrics.csv\", \"ws_sweep.csv\", \"ablation_summary.csv\"]\n",
    "    for r in required:\n",
    "        print(\" -\", r, \"=>\", (run_dir / r).exists())\n",
    "    cm_files = list(run_dir.glob(\"cm_*.png\"))\n",
    "    print(\" - cm_*.png =>\", len(cm_files), \"file(s)\")\n",
    "    if cm_files:\n",
    "        print(\"   \", cm_files[0].name)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "'''\n",
    "(ROOT / \"experiments\" / \"run_all.py\").write_text(run_all_code, encoding=\"utf-8\")\n",
    "\n",
    "print(\"DONE: created files:\")\n",
    "print(\"-\", ROOT / \"experiments\" / \"run_all.py\")\n",
    "print(\"-\", ROOT / \"configs\" / \"phase12_run_all.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d802e693-c17b-4c2b-ab9f-1d5653437ecc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
