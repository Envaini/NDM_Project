{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd6ba327-5b42-4f1f-88a0-5f115a61cb71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CWD = C:\\Users\\ADMIN\\Desktop\\NDM_Project\n",
      "USERSITE in sys.path? False\n",
      "PYTHON = C:\\Users\\ADMIN\\.conda\\envs\\ndm_phase9\\python.exe\n"
     ]
    }
   ],
   "source": [
    "import os, sys, site\n",
    "PROJECT_ROOT = r\"C:\\Users\\ADMIN\\Desktop\\NDM_Project\"\n",
    "os.chdir(PROJECT_ROOT)\n",
    "print(\"CWD =\", os.getcwd())\n",
    "\n",
    "# chặn user-site (đỡ bị numpy/site-packages lẫn)\n",
    "usp = site.getusersitepackages()\n",
    "if usp in sys.path:\n",
    "    sys.path.remove(usp)\n",
    "print(\"USERSITE in sys.path?\", usp in sys.path)\n",
    "print(\"PYTHON =\", sys.executable)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c054921f-72af-4400-9f67-5701967f8914",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting original_paper/method_phase10_domain_shift.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile original_paper/method_phase10_domain_shift.py\n",
    "import os, re, json, math\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io as sio\n",
    "import scipy.signal as sps\n",
    "import scipy.linalg as la\n",
    "\n",
    "# --------------------------\n",
    "# Utils\n",
    "# --------------------------\n",
    "def seed_everything(seed: int = 42):\n",
    "    import random\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "def ensure_dir(p: Path):\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "    return p\n",
    "\n",
    "def l2norm(X: np.ndarray, eps: float = 1e-12):\n",
    "    n = np.linalg.norm(X, axis=1, keepdims=True)\n",
    "    return X / (n + eps)\n",
    "\n",
    "def parse_load_from_name(name: str):\n",
    "    # bắt suffix _<digit> trước .mat hoặc cuối chuỗi\n",
    "    m = re.search(r\"_(\\d+)(?:\\.mat)?$\", name)\n",
    "    return int(m.group(1)) if m else None\n",
    "\n",
    "def parse_label_from_name(name: str):\n",
    "    base = name.replace(\".mat\", \"\")\n",
    "    if base.startswith(\"Normal\"):\n",
    "        return \"H\"\n",
    "    if base.startswith(\"B007\"): return \"B007\"\n",
    "    if base.startswith(\"B014\"): return \"B014\"\n",
    "    if base.startswith(\"B021\"): return \"B021\"\n",
    "    if base.startswith(\"IR007\"): return \"IR007\"\n",
    "    if base.startswith(\"IR014\"): return \"IR014\"\n",
    "    if base.startswith(\"IR021\"): return \"IR021\"\n",
    "    # gom OR theo size, bỏ qua @3/@6/@12\n",
    "    if base.startswith(\"OR007\"): return \"OR007\"\n",
    "    if base.startswith(\"OR014\"): return \"OR014\"\n",
    "    if base.startswith(\"OR021\"): return \"OR021\"\n",
    "    return None\n",
    "\n",
    "def load_cwru_de_time(mat_path: Path):\n",
    "    d = sio.loadmat(mat_path, squeeze_me=True)\n",
    "    # tìm key kiểu *_DE_time\n",
    "    cand = None\n",
    "    for k in d.keys():\n",
    "        if k.endswith(\"_DE_time\"):\n",
    "            cand = k\n",
    "            break\n",
    "    if cand is None:\n",
    "        raise KeyError(f\"Không tìm thấy *_DE_time trong {mat_path.name}. Keys: {list(d.keys())[:20]}\")\n",
    "    x = d[cand].astype(np.float32)\n",
    "    x = np.ravel(x)\n",
    "    return x\n",
    "\n",
    "def sample_segments(x: np.ndarray, seg_len: int, n_seg: int, rng: np.random.Generator):\n",
    "    if len(x) <= seg_len:\n",
    "        # pad nếu quá ngắn\n",
    "        pad = seg_len - len(x) + 1\n",
    "        x = np.pad(x, (0, pad), mode=\"wrap\")\n",
    "    max_start = len(x) - seg_len\n",
    "    starts = rng.integers(0, max_start + 1, size=n_seg)\n",
    "    segs = np.stack([x[s:s+seg_len] for s in starts], axis=0)\n",
    "    return segs\n",
    "\n",
    "# --------------------------\n",
    "# Log-mel (paper-like): output (96, 64)\n",
    "# --------------------------\n",
    "def logmel_96x64(wave: np.ndarray, sr: int = 48000, n_fft: int = 1024, hop: int = 256, n_mels: int = 64, n_frames: int = 96):\n",
    "    import librosa\n",
    "    # normalize nhẹ\n",
    "    w = wave.astype(np.float32)\n",
    "    w = w - w.mean()\n",
    "    w = w / (np.max(np.abs(w)) + 1e-9)\n",
    "\n",
    "    S = librosa.feature.melspectrogram(y=w, sr=sr, n_fft=n_fft, hop_length=hop, n_mels=n_mels, power=2.0)\n",
    "    logS = librosa.power_to_db(S, ref=np.max)  # (n_mels, T)\n",
    "\n",
    "    # chuyển thành (T, n_mels)\n",
    "    img = logS.T  # (T, 64)\n",
    "\n",
    "    # crop/pad về 96 frame\n",
    "    T = img.shape[0]\n",
    "    if T >= n_frames:\n",
    "        img = img[:n_frames, :]\n",
    "    else:\n",
    "        pad = n_frames - T\n",
    "        img = np.pad(img, ((0, pad), (0, 0)), mode=\"edge\")\n",
    "\n",
    "    # scale về 0..1 cho ổn định LPQ\n",
    "    mn, mx = img.min(), img.max()\n",
    "    img = (img - mn) / (mx - mn + 1e-9)\n",
    "    return img.astype(np.float32)  # (96, 64)\n",
    "\n",
    "# --------------------------\n",
    "# LPQ + MBH-LPQ\n",
    "# --------------------------\n",
    "def lpq_codes(img: np.ndarray, R: int = 7):\n",
    "    # img: (H,W) float\n",
    "    a = 1.0 / (2*R + 1)\n",
    "    x = np.arange(-R, R+1)\n",
    "    y = np.arange(-R, R+1)\n",
    "    X, Y = np.meshgrid(x, y, indexing=\"xy\")\n",
    "\n",
    "    w1 = np.exp(-2j*np.pi*a*X)\n",
    "    w2 = np.exp(-2j*np.pi*a*Y)\n",
    "\n",
    "    f1 = w1\n",
    "    f2 = w2\n",
    "    f3 = w1*w2\n",
    "    f4 = w1*np.conj(w2)\n",
    "\n",
    "    def conv_complex(f):\n",
    "        re = sps.convolve2d(img, np.real(f), mode=\"same\", boundary=\"symm\")\n",
    "        im = sps.convolve2d(img, np.imag(f), mode=\"same\", boundary=\"symm\")\n",
    "        return re, im\n",
    "\n",
    "    re1, im1 = conv_complex(f1)\n",
    "    re2, im2 = conv_complex(f2)\n",
    "    re3, im3 = conv_complex(f3)\n",
    "    re4, im4 = conv_complex(f4)\n",
    "\n",
    "    bits = [\n",
    "        (re1 > 0), (im1 > 0),\n",
    "        (re2 > 0), (im2 > 0),\n",
    "        (re3 > 0), (im3 > 0),\n",
    "        (re4 > 0), (im4 > 0),\n",
    "    ]\n",
    "    code = np.zeros(img.shape, dtype=np.uint8)\n",
    "    for i, b in enumerate(bits):\n",
    "        code |= (b.astype(np.uint8) << i)\n",
    "    return code  # (H,W) uint8 0..255\n",
    "\n",
    "def best_grid(b: int):\n",
    "    # tìm (gh,gw) sao cho gh*gw=b và gh gần gw nhất\n",
    "    best = (1, b, 10**9)\n",
    "    for gh in range(1, b+1):\n",
    "        if b % gh == 0:\n",
    "            gw = b // gh\n",
    "            score = abs(gh - gw)\n",
    "            if score < best[2]:\n",
    "                best = (gh, gw, score)\n",
    "    return best[0], best[1]\n",
    "\n",
    "def mbh_lpq_feature(code_img: np.ndarray, b: int):\n",
    "    H, W = code_img.shape\n",
    "    gh, gw = best_grid(b)  # vd b=10 -> (2,5)\n",
    "    feats = []\n",
    "    for i in range(gh):\n",
    "        for j in range(gw):\n",
    "            r0 = int(round(i * H / gh))\n",
    "            r1 = int(round((i+1) * H / gh))\n",
    "            c0 = int(round(j * W / gw))\n",
    "            c1 = int(round((j+1) * W / gw))\n",
    "            block = code_img[r0:r1, c0:c1]\n",
    "            hist = np.bincount(block.ravel(), minlength=256).astype(np.float32)\n",
    "            hist = hist / (hist.sum() + 1e-9)\n",
    "            feats.append(hist)\n",
    "    return np.concatenate(feats, axis=0)  # (b*256,)\n",
    "\n",
    "# --------------------------\n",
    "# PCA + EDA\n",
    "# --------------------------\n",
    "def fit_pca_eda(Xtr: np.ndarray, ytr: np.ndarray, pca_dim: int = 128, out_dim: int = None, reg: float = 1e-6):\n",
    "    scaler = StandardScaler()\n",
    "    Xs = scaler.fit_transform(Xtr)\n",
    "\n",
    "    d = Xs.shape[1]\n",
    "    n = Xs.shape[0]\n",
    "    p = min(pca_dim, d, max(2, n-1))\n",
    "    pca = PCA(n_components=p, random_state=0)\n",
    "    Xp = pca.fit_transform(Xs)\n",
    "\n",
    "    # EDA: expm(Sb)v = λ expm(Sw)v\n",
    "    classes = np.unique(ytr)\n",
    "    mu = Xp.mean(axis=0, keepdims=True)\n",
    "\n",
    "    Sw = np.zeros((p, p), dtype=np.float64)\n",
    "    Sb = np.zeros((p, p), dtype=np.float64)\n",
    "\n",
    "    for c in classes:\n",
    "        Xc = Xp[ytr == c]\n",
    "        muc = Xc.mean(axis=0, keepdims=True)\n",
    "        Sw += (Xc - muc).T @ (Xc - muc)\n",
    "        Sb += Xc.shape[0] * (muc - mu).T @ (muc - mu)\n",
    "\n",
    "    Sw = Sw / max(1, (Xp.shape[0] - len(classes)))\n",
    "    Sb = Sb / max(1, len(classes))\n",
    "    Sw += reg * np.eye(p)\n",
    "\n",
    "    A = la.expm(Sb)\n",
    "    B = la.expm(Sw)\n",
    "    A = (A + A.T) / 2.0\n",
    "    B = (B + B.T) / 2.0\n",
    "\n",
    "    # solve generalized eigen\n",
    "    w, V = la.eigh(A, B)\n",
    "    idx = np.argsort(w)[::-1]\n",
    "    V = V[:, idx]\n",
    "\n",
    "    if out_dim is None:\n",
    "        out_dim = min(len(classes) - 1, p)\n",
    "        out_dim = max(2, out_dim)\n",
    "    W = V[:, :out_dim].astype(np.float32)\n",
    "\n",
    "    def transform(X: np.ndarray):\n",
    "        Xs2 = scaler.transform(X)\n",
    "        Xp2 = pca.transform(Xs2)\n",
    "        Z = Xp2 @ W\n",
    "        return Z.astype(np.float32)\n",
    "\n",
    "    return transform\n",
    "\n",
    "# --------------------------\n",
    "# Cosine-prototype classifier\n",
    "# --------------------------\n",
    "def fit_prototypes(Xtr: np.ndarray, ytr: np.ndarray):\n",
    "    Xn = l2norm(Xtr)\n",
    "    classes = np.unique(ytr)\n",
    "    protos = []\n",
    "    for c in classes:\n",
    "        pc = Xn[ytr == c].mean(axis=0, keepdims=True)\n",
    "        pc = l2norm(pc)\n",
    "        protos.append(pc)\n",
    "    P = np.concatenate(protos, axis=0)  # (C,d)\n",
    "    return classes, P\n",
    "\n",
    "def predict_with_protos(X: np.ndarray, classes: np.ndarray, P: np.ndarray):\n",
    "    Xn = l2norm(X)\n",
    "    scores = Xn @ P.T  # cosine similarity\n",
    "    pred = classes[np.argmax(scores, axis=1)]\n",
    "    return pred, scores\n",
    "\n",
    "# --------------------------\n",
    "# Main experiment\n",
    "# --------------------------\n",
    "def scan_metadata(cwru_root: Path):\n",
    "    rows = []\n",
    "    fault_dir = cwru_root / \"48k_drive_end_fault\"\n",
    "    normal_dir = cwru_root / \"normal_baseline\"\n",
    "\n",
    "    for p in list(fault_dir.glob(\"*.mat\")) + list(normal_dir.glob(\"*.mat\")):\n",
    "        name = p.name\n",
    "        y = parse_label_from_name(name)\n",
    "        ld = parse_load_from_name(name)\n",
    "        if y is None or ld is None:\n",
    "            continue\n",
    "        rows.append({\"path\": str(p), \"name\": name, \"label\": y, \"load\": ld})\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    if df.empty:\n",
    "        raise RuntimeError(\"Không scan được file *.mat theo format *_<load>.mat. Kiểm tra tên file và folder.\")\n",
    "    return df\n",
    "\n",
    "def build_domain_samples(df: pd.DataFrame, load: int, n_per_class: int, seg_len: int, sr: int, seed: int):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    sub = df[df[\"load\"] == load].copy()\n",
    "    labels = sorted(sub[\"label\"].unique().tolist())\n",
    "\n",
    "    X_wave = []\n",
    "    y = []\n",
    "\n",
    "    for lab in labels:\n",
    "        files = sub[sub[\"label\"] == lab][\"path\"].tolist()\n",
    "        if len(files) == 0:\n",
    "            continue\n",
    "        need = n_per_class\n",
    "        per_file = max(1, math.ceil(need / len(files)))\n",
    "        got = 0\n",
    "        for fp in files:\n",
    "            x = load_cwru_de_time(Path(fp))\n",
    "            segs = sample_segments(x, seg_len=seg_len, n_seg=per_file, rng=rng)\n",
    "            take = min(segs.shape[0], need - got)\n",
    "            X_wave.append(segs[:take])\n",
    "            y += [lab] * take\n",
    "            got += take\n",
    "            if got >= need:\n",
    "                break\n",
    "\n",
    "    X_wave = np.concatenate(X_wave, axis=0).astype(np.float32)\n",
    "    y = np.array(y)\n",
    "    # shuffle\n",
    "    idx = rng.permutation(len(y))\n",
    "    return X_wave[idx], y[idx]\n",
    "\n",
    "def run_one_pair(df, train_load, test_load, outdir: Path,\n",
    "                 n_per_class=120, seg_len=4800, sr=48000,\n",
    "                 lpq_R=7, b_list=(1,2,4,6,8,10,12),\n",
    "                 alpha_list=(0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9),\n",
    "                 seed=42):\n",
    "    ensure_dir(outdir)\n",
    "    seed_everything(seed)\n",
    "\n",
    "    # 1) build waveform samples\n",
    "    Xtr_wave, ytr = build_domain_samples(df, train_load, n_per_class, seg_len, sr, seed=seed)\n",
    "    Xte_wave, yte = build_domain_samples(df, test_load,  n_per_class, seg_len, sr, seed=seed+123)\n",
    "\n",
    "    # split val from train domain\n",
    "    Xtr_wave, Xval_wave, ytr, yval = train_test_split(\n",
    "        Xtr_wave, ytr, test_size=0.2, random_state=seed, stratify=ytr\n",
    "    )\n",
    "\n",
    "    # 2) feature extraction: VGGish + MBH-LPQ(b sweep)\n",
    "    # NOTE: dùng torchaudio VGGISH nếu có; nếu không thì chạy branch shallow vẫn được.\n",
    "    deep_ok = True\n",
    "    try:\n",
    "        import torch\n",
    "        import torchaudio\n",
    "        from torchaudio.prototype.pipelines import VGGISH\n",
    "        bundle = VGGISH\n",
    "        model = bundle.get_model()\n",
    "        model.eval()\n",
    "        iproc = bundle.get_input_processor()\n",
    "        resampler = torchaudio.transforms.Resample(orig_freq=sr, new_freq=16000)\n",
    "    except Exception as e:\n",
    "        deep_ok = False\n",
    "        model = None\n",
    "        iproc = None\n",
    "        resampler = None\n",
    "        print(\"[WARN] Không load được torchaudio VGGISH -> bỏ deep branch. Lỗi:\", repr(e))\n",
    "\n",
    "    def vggish_embed_batch(Xwave: np.ndarray):\n",
    "        if not deep_ok:\n",
    "            return None\n",
    "        import torch\n",
    "        feats = []\n",
    "        with torch.no_grad():\n",
    "            for w in Xwave:\n",
    "                t = torch.from_numpy(w).float().unsqueeze(0)\n",
    "                t = t - t.mean()\n",
    "                t = t / (t.abs().max() + 1e-9)\n",
    "                t16 = resampler(t)\n",
    "                # iproc signature khác nhau theo version\n",
    "                try:\n",
    "                    inp = iproc(t16)\n",
    "                except TypeError:\n",
    "                    inp = iproc(t16, sample_rate=16000)\n",
    "                out = model(inp)\n",
    "                # out shape có thể (B,T,128) hoặc (B,128)\n",
    "                if out.ndim == 3:\n",
    "                    v = out.mean(dim=1).squeeze(0)\n",
    "                else:\n",
    "                    v = out.squeeze(0)\n",
    "                feats.append(v.cpu().numpy().astype(np.float32))\n",
    "        return np.stack(feats, axis=0)\n",
    "\n",
    "    def shallow_feats_for_wave_batch(Xwave: np.ndarray):\n",
    "        # trả dict b -> (N, b*256)\n",
    "        feats_b = {b: [] for b in b_list}\n",
    "        # audit: lưu 1 logmel ví dụ\n",
    "        audit_saved = False\n",
    "\n",
    "        for w in Xwave:\n",
    "            img = logmel_96x64(w, sr=sr)  # (96,64)\n",
    "            if not audit_saved:\n",
    "                plt.figure()\n",
    "                plt.imshow(img.T, aspect=\"auto\", origin=\"lower\")\n",
    "                plt.title(\"Audit log-mel (paper-like)\")\n",
    "                plt.tight_layout()\n",
    "                audit_dir = ensure_dir(outdir / \"audit\")\n",
    "                plt.savefig(audit_dir / \"logmel_example.png\", dpi=200)\n",
    "                plt.close()\n",
    "                audit_saved = True\n",
    "\n",
    "            code = lpq_codes(img, R=lpq_R)\n",
    "            for b in b_list:\n",
    "                feats_b[b].append(mbh_lpq_feature(code, b=b))\n",
    "\n",
    "        for b in b_list:\n",
    "            feats_b[b] = np.stack(feats_b[b], axis=0).astype(np.float32)\n",
    "        return feats_b\n",
    "\n",
    "    print(f\"[Phase10] Extract shallow (MBH-LPQ) train/val/test ...\")\n",
    "    tr_sh = shallow_feats_for_wave_batch(Xtr_wave)\n",
    "    va_sh = shallow_feats_for_wave_batch(Xval_wave)\n",
    "    te_sh = shallow_feats_for_wave_batch(Xte_wave)\n",
    "\n",
    "    Xtr_de = Xval_de = Xte_de = None\n",
    "    if deep_ok:\n",
    "        print(f\"[Phase10] Extract deep (VGGish) train/val/test ...\")\n",
    "        Xtr_de = vggish_embed_batch(Xtr_wave)\n",
    "        Xval_de = vggish_embed_batch(Xval_wave)\n",
    "        Xte_de = vggish_embed_batch(Xte_wave)\n",
    "\n",
    "    # 3) train/eval\n",
    "    rows = []\n",
    "    best = {\"best_b\": None, \"best_alpha\": None, \"acc_val\": -1}\n",
    "\n",
    "    # deep-only pipeline\n",
    "    if deep_ok:\n",
    "        de_tf = fit_pca_eda(Xtr_de, ytr, pca_dim=128)\n",
    "        Ztr_de = de_tf(Xtr_de); Zva_de = de_tf(Xval_de); Zte_de = de_tf(Xte_de)\n",
    "        cls, P = fit_prototypes(Ztr_de, ytr)\n",
    "        pv, sv = predict_with_protos(Zva_de, cls, P)\n",
    "        pt, st = predict_with_protos(Zte_de, cls, P)\n",
    "        rows.append({\"model\": \"VGGish\", \"b\": None, \"alpha\": 1.0,\n",
    "                     \"acc_val\": accuracy_score(yval, pv), \"acc_test\": accuracy_score(yte, pt),\n",
    "                     \"f1_val\": f1_score(yval, pv, average=\"macro\"), \"f1_test\": f1_score(yte, pt, average=\"macro\")})\n",
    "    else:\n",
    "        sv = st = None\n",
    "\n",
    "    # sweep b for shallow + fusion alpha\n",
    "    for b in b_list:\n",
    "        sh_tf = fit_pca_eda(tr_sh[b], ytr, pca_dim=256)\n",
    "        Ztr_sh = sh_tf(tr_sh[b]); Zva_sh = sh_tf(va_sh[b]); Zte_sh = sh_tf(te_sh[b])\n",
    "\n",
    "        cls2, P2 = fit_prototypes(Ztr_sh, ytr)\n",
    "        pv_sh, sv_sh = predict_with_protos(Zva_sh, cls2, P2)\n",
    "        pt_sh, st_sh = predict_with_protos(Zte_sh, cls2, P2)\n",
    "\n",
    "        rows.append({\"model\": \"MBH-LPQ\", \"b\": b, \"alpha\": 0.0,\n",
    "                     \"acc_val\": accuracy_score(yval, pv_sh), \"acc_test\": accuracy_score(yte, pt_sh),\n",
    "                     \"f1_val\": f1_score(yval, pv_sh, average=\"macro\"), \"f1_test\": f1_score(yte, pt_sh, average=\"macro\")})\n",
    "\n",
    "        # fusion\n",
    "        if deep_ok:\n",
    "            for alpha in alpha_list:\n",
    "                # score = alpha*deep + (1-alpha)*shallow\n",
    "                # deep score dùng cls/P; shallow score dùng cls2/P2 (cần cùng order class)\n",
    "                # ép về cùng thứ tự class = sorted union\n",
    "                all_cls = sorted(set(cls.tolist()) | set(cls2.tolist()))\n",
    "                all_cls = np.array(all_cls)\n",
    "\n",
    "                def align_scores(scores, cls_src):\n",
    "                    m = {c:i for i,c in enumerate(cls_src)}\n",
    "                    out = np.zeros((scores.shape[0], len(all_cls)), dtype=np.float32)\n",
    "                    for j,c in enumerate(all_cls):\n",
    "                        if c in m:\n",
    "                            out[:, j] = scores[:, m[c]]\n",
    "                        else:\n",
    "                            out[:, j] = -1e9\n",
    "                    return out\n",
    "\n",
    "                Sv_de = align_scores(sv, cls)    # (N,C)\n",
    "                Sv_sh = align_scores(sv_sh, cls2)\n",
    "                St_de = align_scores(st, cls)\n",
    "                St_sh = align_scores(st_sh, cls2)\n",
    "\n",
    "                Sv = alpha*Sv_de + (1-alpha)*Sv_sh\n",
    "                St = alpha*St_de + (1-alpha)*St_sh\n",
    "\n",
    "                pv = all_cls[np.argmax(Sv, axis=1)]\n",
    "                pt = all_cls[np.argmax(St, axis=1)]\n",
    "\n",
    "                accv = accuracy_score(yval, pv)\n",
    "                acct = accuracy_score(yte, pt)\n",
    "\n",
    "                rows.append({\"model\": \"Fusion\", \"b\": b, \"alpha\": float(alpha),\n",
    "                             \"acc_val\": accv, \"acc_test\": acct,\n",
    "                             \"f1_val\": f1_score(yval, pv, average=\"macro\"),\n",
    "                             \"f1_test\": f1_score(yte, pt, average=\"macro\")})\n",
    "\n",
    "                if accv > best[\"acc_val\"]:\n",
    "                    best.update({\"best_b\": b, \"best_alpha\": float(alpha), \"acc_val\": float(accv)})\n",
    "\n",
    "    dfm = pd.DataFrame(rows)\n",
    "    dfm.to_csv(outdir / \"domain_shift_metrics.csv\", index=False)\n",
    "\n",
    "    # best.json\n",
    "    (outdir / \"best.json\").write_text(json.dumps(best, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "    # confusion matrix for best fusion (or best shallow if deep not ok)\n",
    "    if deep_ok and best[\"best_b\"] is not None:\n",
    "        # rebuild best to plot confusion matrix\n",
    "        b = best[\"best_b\"]; alpha = best[\"best_alpha\"]\n",
    "        sh_tf = fit_pca_eda(tr_sh[b], ytr, pca_dim=256)\n",
    "        Ztr_sh = sh_tf(tr_sh[b]); Zte_sh = sh_tf(te_sh[b])\n",
    "        cls2, P2 = fit_prototypes(Ztr_sh, ytr)\n",
    "        _, St_sh = predict_with_protos(Zte_sh, cls2, P2)\n",
    "\n",
    "        de_tf = fit_pca_eda(Xtr_de, ytr, pca_dim=128)\n",
    "        Ztr_de = de_tf(Xtr_de); Zte_de = de_tf(Xte_de)\n",
    "        cls, P = fit_prototypes(Ztr_de, ytr)\n",
    "        _, St_de = predict_with_protos(Zte_de, cls, P)\n",
    "\n",
    "        all_cls = np.array(sorted(set(cls.tolist()) | set(cls2.tolist())))\n",
    "\n",
    "        def align(scores, cls_src):\n",
    "            m = {c:i for i,c in enumerate(cls_src)}\n",
    "            out = np.zeros((scores.shape[0], len(all_cls)), dtype=np.float32)\n",
    "            for j,c in enumerate(all_cls):\n",
    "                out[:, j] = scores[:, m[c]] if c in m else -1e9\n",
    "            return out\n",
    "\n",
    "        St = alpha*align(St_de, cls) + (1-alpha)*align(St_sh, cls2)\n",
    "        ypred = all_cls[np.argmax(St, axis=1)]\n",
    "    else:\n",
    "        # shallow best b\n",
    "        b = dfm[dfm[\"model\"]==\"MBH-LPQ\"].sort_values(\"acc_val\", ascending=False).iloc[0][\"b\"]\n",
    "        sh_tf = fit_pca_eda(tr_sh[int(b)], ytr, pca_dim=256)\n",
    "        Ztr_sh = sh_tf(tr_sh[int(b)]); Zte_sh = sh_tf(te_sh[int(b)])\n",
    "        cls2, P2 = fit_prototypes(Ztr_sh, ytr)\n",
    "        ypred, _ = predict_with_protos(Zte_sh, cls2, P2)\n",
    "        all_cls = np.array(sorted(set(ytr.tolist())))\n",
    "\n",
    "    cm = confusion_matrix(yte, ypred, labels=all_cls)\n",
    "\n",
    "    plt.figure(figsize=(7,6))\n",
    "    plt.imshow(cm, aspect=\"auto\")\n",
    "    plt.title(f\"Confusion Matrix (train_load={train_load}, test_load={test_load})\")\n",
    "    plt.xticks(range(len(all_cls)), all_cls, rotation=45, ha=\"right\")\n",
    "    plt.yticks(range(len(all_cls)), all_cls)\n",
    "    plt.colorbar()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(outdir / \"cm_domain_shift.png\", dpi=200)\n",
    "    plt.close()\n",
    "\n",
    "    # quick plots: best of fusion vs b, alpha\n",
    "    # b_sweep plot (use fusion at best alpha per b, lấy max val)\n",
    "    fus = dfm[dfm[\"model\"]==\"Fusion\"].copy()\n",
    "    if not fus.empty:\n",
    "        bs = fus.groupby(\"b\")[[\"acc_val\",\"acc_test\"]].max().reset_index()\n",
    "        bs.to_csv(outdir / \"b_sweep.csv\", index=False)\n",
    "        plt.figure()\n",
    "        plt.plot(bs[\"b\"], bs[\"acc_val\"], marker=\"o\", label=\"val\")\n",
    "        plt.plot(bs[\"b\"], bs[\"acc_test\"], marker=\"o\", label=\"test\")\n",
    "        plt.xlabel(\"b (sub-blocks)\"); plt.ylabel(\"Accuracy\"); plt.title(\"b_sweep\")\n",
    "        plt.legend(); plt.tight_layout()\n",
    "        plt.savefig(outdir / \"b_sweep_plot.png\", dpi=200); plt.close()\n",
    "\n",
    "        ws = fus.groupby(\"alpha\")[[\"acc_val\",\"acc_test\"]].max().reset_index()\n",
    "        ws.to_csv(outdir / \"ws_sweep.csv\", index=False)\n",
    "        plt.figure()\n",
    "        plt.plot(ws[\"alpha\"], ws[\"acc_val\"], marker=\"o\", label=\"val\")\n",
    "        plt.plot(ws[\"alpha\"], ws[\"acc_test\"], marker=\"o\", label=\"test\")\n",
    "        plt.xlabel(\"alpha (weight deep)\"); plt.ylabel(\"Accuracy\"); plt.title(\"ws_sweep\")\n",
    "        plt.legend(); plt.tight_layout()\n",
    "        plt.savefig(outdir / \"ws_sweep_plot.png\", dpi=200); plt.close()\n",
    "\n",
    "    # return summary row\n",
    "    best_row = dfm[dfm[\"model\"]==\"Fusion\"].sort_values(\"acc_val\", ascending=False).head(1)\n",
    "    if best_row.empty:\n",
    "        # fallback\n",
    "        best_row = dfm.sort_values(\"acc_val\", ascending=False).head(1)\n",
    "\n",
    "    r = best_row.iloc[0].to_dict()\n",
    "    r.update({\"train_load\": train_load, \"test_load\": test_load})\n",
    "    return r\n",
    "\n",
    "def main():\n",
    "    seed = 42\n",
    "    cwru_root = Path(\"data/raw/CWRU\")\n",
    "    out_root = ensure_dir(Path(\"results/phase10\"))\n",
    "\n",
    "    df = scan_metadata(cwru_root)\n",
    "    loads = sorted(df[\"load\"].unique().tolist())\n",
    "    print(\"[Phase10] Loads found:\", loads)\n",
    "    print(df.groupby([\"load\",\"label\"]).size().head(20))\n",
    "\n",
    "    # chạy tất cả cặp load khác nhau + baseline cùng load\n",
    "    summary = []\n",
    "    for tr in loads:\n",
    "        for te in loads:\n",
    "            od = out_root / f\"train{tr}_test{te}\"\n",
    "            print(f\"\\n[Phase10] RUN train_load={tr} -> test_load={te}\")\n",
    "            row = run_one_pair(df, tr, te, od, seed=seed)\n",
    "            summary.append(row)\n",
    "\n",
    "    sdf = pd.DataFrame(summary)\n",
    "    sdf.to_csv(out_root / \"domain_shift_summary.csv\", index=False)\n",
    "\n",
    "    # tạo bảng so sánh dễ nhìn: acc_test của Fusion theo (train,test)\n",
    "    piv = sdf.pivot_table(index=\"train_load\", columns=\"test_load\", values=\"acc_test\", aggfunc=\"max\")\n",
    "    piv.to_csv(out_root / \"domain_shift_matrix.csv\")\n",
    "\n",
    "    print(\"\\n[Phase10] DONE ->\", str(out_root.resolve()))\n",
    "    print(\"- domain_shift_summary.csv\")\n",
    "    print(\"- domain_shift_matrix.csv\")\n",
    "    print(\"- các folder trainA_testB/ chứa cm + sweep + audit\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df07e188-e402-4d4d-92c4-c1334926fd06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Phase10] Loads found: [0, 1, 2, 3]\n",
      "load  label\n",
      "0     B007     1\n",
      "      B014     1\n",
      "      B021     1\n",
      "      H        1\n",
      "      IR007    1\n",
      "      IR014    1\n",
      "      IR021    1\n",
      "      OR007    1\n",
      "      OR014    1\n",
      "      OR021    1\n",
      "1     B007     1\n",
      "      B014     1\n",
      "      B021     1\n",
      "      H        1\n",
      "      IR007    1\n",
      "      IR014    1\n",
      "      IR021    1\n",
      "      OR007    3\n",
      "      OR014    1\n",
      "      OR021    3\n",
      "2     B007     1\n",
      "      B014     1\n",
      "      B021     1\n",
      "      H        1\n",
      "      IR007    1\n",
      "      IR014    1\n",
      "      IR021    1\n",
      "      OR007    1\n",
      "      OR014    1\n",
      "      OR021    1\n",
      "3     B007     1\n",
      "      B014     1\n",
      "      B021     1\n",
      "      H        1\n",
      "      IR007    1\n",
      "      IR014    1\n",
      "      IR021    1\n",
      "      OR007    1\n",
      "      OR014    1\n",
      "      OR021    1\n",
      "dtype: int64\n",
      "\n",
      "[Phase10] RUN train_load=0 -> test_load=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\.conda\\envs\\ndm_phase9\\Lib\\site-packages\\torchaudio\\prototype\\pipelines\\_vggish\\_vggish_pipeline.py:12: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Phase10] Extract shallow (MBH-LPQ) train/val/test ...\n",
      "[Phase10] Extract deep (VGGish) train/val/test ...\n",
      "\n",
      "[Phase10] RUN train_load=0 -> test_load=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\.conda\\envs\\ndm_phase9\\Lib\\site-packages\\torchaudio\\prototype\\pipelines\\_vggish\\_vggish_pipeline.py:12: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Phase10] Extract shallow (MBH-LPQ) train/val/test ...\n",
      "[Phase10] Extract deep (VGGish) train/val/test ...\n",
      "\n",
      "[Phase10] RUN train_load=0 -> test_load=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\.conda\\envs\\ndm_phase9\\Lib\\site-packages\\torchaudio\\prototype\\pipelines\\_vggish\\_vggish_pipeline.py:12: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Phase10] Extract shallow (MBH-LPQ) train/val/test ...\n",
      "[Phase10] Extract deep (VGGish) train/val/test ...\n",
      "\n",
      "[Phase10] RUN train_load=0 -> test_load=3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\.conda\\envs\\ndm_phase9\\Lib\\site-packages\\torchaudio\\prototype\\pipelines\\_vggish\\_vggish_pipeline.py:12: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Phase10] Extract shallow (MBH-LPQ) train/val/test ...\n",
      "[Phase10] Extract deep (VGGish) train/val/test ...\n",
      "\n",
      "[Phase10] RUN train_load=1 -> test_load=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\.conda\\envs\\ndm_phase9\\Lib\\site-packages\\torchaudio\\prototype\\pipelines\\_vggish\\_vggish_pipeline.py:12: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Phase10] Extract shallow (MBH-LPQ) train/val/test ...\n",
      "[Phase10] Extract deep (VGGish) train/val/test ...\n",
      "\n",
      "[Phase10] RUN train_load=1 -> test_load=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\.conda\\envs\\ndm_phase9\\Lib\\site-packages\\torchaudio\\prototype\\pipelines\\_vggish\\_vggish_pipeline.py:12: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Phase10] Extract shallow (MBH-LPQ) train/val/test ...\n",
      "[Phase10] Extract deep (VGGish) train/val/test ...\n",
      "\n",
      "[Phase10] RUN train_load=1 -> test_load=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\.conda\\envs\\ndm_phase9\\Lib\\site-packages\\torchaudio\\prototype\\pipelines\\_vggish\\_vggish_pipeline.py:12: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Phase10] Extract shallow (MBH-LPQ) train/val/test ...\n",
      "[Phase10] Extract deep (VGGish) train/val/test ...\n",
      "\n",
      "[Phase10] RUN train_load=1 -> test_load=3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\.conda\\envs\\ndm_phase9\\Lib\\site-packages\\torchaudio\\prototype\\pipelines\\_vggish\\_vggish_pipeline.py:12: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Phase10] Extract shallow (MBH-LPQ) train/val/test ...\n",
      "[Phase10] Extract deep (VGGish) train/val/test ...\n",
      "\n",
      "[Phase10] RUN train_load=2 -> test_load=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\.conda\\envs\\ndm_phase9\\Lib\\site-packages\\torchaudio\\prototype\\pipelines\\_vggish\\_vggish_pipeline.py:12: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Phase10] Extract shallow (MBH-LPQ) train/val/test ...\n",
      "[Phase10] Extract deep (VGGish) train/val/test ...\n",
      "\n",
      "[Phase10] RUN train_load=2 -> test_load=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\.conda\\envs\\ndm_phase9\\Lib\\site-packages\\torchaudio\\prototype\\pipelines\\_vggish\\_vggish_pipeline.py:12: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Phase10] Extract shallow (MBH-LPQ) train/val/test ...\n",
      "[Phase10] Extract deep (VGGish) train/val/test ...\n",
      "\n",
      "[Phase10] RUN train_load=2 -> test_load=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\.conda\\envs\\ndm_phase9\\Lib\\site-packages\\torchaudio\\prototype\\pipelines\\_vggish\\_vggish_pipeline.py:12: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Phase10] Extract shallow (MBH-LPQ) train/val/test ...\n",
      "[Phase10] Extract deep (VGGish) train/val/test ...\n",
      "\n",
      "[Phase10] RUN train_load=2 -> test_load=3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\.conda\\envs\\ndm_phase9\\Lib\\site-packages\\torchaudio\\prototype\\pipelines\\_vggish\\_vggish_pipeline.py:12: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Phase10] Extract shallow (MBH-LPQ) train/val/test ...\n",
      "[Phase10] Extract deep (VGGish) train/val/test ...\n",
      "\n",
      "[Phase10] RUN train_load=3 -> test_load=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\.conda\\envs\\ndm_phase9\\Lib\\site-packages\\torchaudio\\prototype\\pipelines\\_vggish\\_vggish_pipeline.py:12: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Phase10] Extract shallow (MBH-LPQ) train/val/test ...\n",
      "[Phase10] Extract deep (VGGish) train/val/test ...\n",
      "\n",
      "[Phase10] RUN train_load=3 -> test_load=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\.conda\\envs\\ndm_phase9\\Lib\\site-packages\\torchaudio\\prototype\\pipelines\\_vggish\\_vggish_pipeline.py:12: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Phase10] Extract shallow (MBH-LPQ) train/val/test ...\n",
      "[Phase10] Extract deep (VGGish) train/val/test ...\n",
      "\n",
      "[Phase10] RUN train_load=3 -> test_load=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\.conda\\envs\\ndm_phase9\\Lib\\site-packages\\torchaudio\\prototype\\pipelines\\_vggish\\_vggish_pipeline.py:12: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Phase10] Extract shallow (MBH-LPQ) train/val/test ...\n",
      "[Phase10] Extract deep (VGGish) train/val/test ...\n",
      "\n",
      "[Phase10] RUN train_load=3 -> test_load=3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\.conda\\envs\\ndm_phase9\\Lib\\site-packages\\torchaudio\\prototype\\pipelines\\_vggish\\_vggish_pipeline.py:12: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Phase10] Extract shallow (MBH-LPQ) train/val/test ...\n",
      "[Phase10] Extract deep (VGGish) train/val/test ...\n",
      "\n",
      "[Phase10] DONE -> C:\\Users\\ADMIN\\Desktop\\NDM_Project\\results\\phase10\n",
      "- domain_shift_summary.csv\n",
      "- domain_shift_matrix.csv\n",
      "- các folder trainA_testB/ chứa cm + sweep + audit\n"
     ]
    }
   ],
   "source": [
    "%run original_paper/method_phase10_domain_shift.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3b34d9-10dd-4b38-bc4b-3ef5a7322d1e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
